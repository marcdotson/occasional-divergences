[
  {
    "objectID": "posts/stan-hierarchical/index.html",
    "href": "posts/stan-hierarchical/index.html",
    "title": "Hierarchical models in Stan",
    "section": "",
    "text": "Bayesian inference provides an intuitive and self-consistent approach to statistical modeling. In short, you have beliefs about unobserved values (e.g., the impact of price on customer satisfaction) and you use data to update those beliefs. The barrier to using Bayesian inference in practice has never been this intuition – it’s been the required computation. There has been a great deal of progress on this front, with Stan arguably providing the best modern solution.\nStan is a probabilistic programming language that provides a general-purpose sampler using Hamiltonian Monte Carlo. In other words, Stan automates the required computation (for many models), allowing you to conduct Bayesian inference by focusing solely on model building. This is especially powerful when it comes to utilizing the mainstay of Bayesian inference: hierarchical models.\nThe goal of this post is to provide a gentle introduction to building hierarchical models in Stan. We will be interfacing with Stan through R and will be adopting a tidy approach whenever appropriate. This post does not provide an introduction to Bayesian inference, including a complete Bayesian inference workflow, or the basics of using Stan. For that, I recommend Michael Betancourt’s case studies, Richard McElreath’s Statistical Rethinking, and the Stan User’s Guide. Much of what follows is motivated by Michael and Richard’s work as well as Ben Goodrich’s StanCon 2019 tutorial."
  },
  {
    "objectID": "posts/stan-hierarchical/index.html#simple-flat-regression",
    "href": "posts/stan-hierarchical/index.html#simple-flat-regression",
    "title": "Hierarchical models in Stan",
    "section": "Simple flat regression",
    "text": "Simple flat regression\nBy way of introduction, let’s start with a simple flat or non-hierarchical regression. In a Stan script, which has native support in RStudio, we specify the three required blocks for a Stan model: data, parameters, and model (i.e., the prior and the likelihood or observation model).\n\n// Index value and observations.\ndata {\n  int<lower = 1> N;    // Number of observations.\n  vector[N] y;         // Vector of observations.\n}\n\n// Parameters.\nparameters {\n  real mu;             // Mean of the regression.\n  real<lower = 0> tau; // Variance of the regression.\n}\n\n// Regression.\nmodel {\n  // Priors.\n  mu ~ normal(0, 5);\n  tau ~ normal(0, 5);\n\n  // Likelihood.\n  y ~ normal(mu, tau);\n}\n\nWe’ll save this Stan script as flat_regression.stan. Imagine this is a model of customer satisfaction where we have N individuals, a vector of satisfaction scores y with a single observation from each individual, and we are assuming that satisfaction y is distributed normal, with a single mean mu and variance tau to describe customer satisfaction in the population. Finally, our joint model is complete with normal priors on mu and tau, where tau is constrained to be positive in the parameters block.\nLike all Bayesian models, this model is generative, so we can also use Stan to generate data according to some assumed parameter values and then use the generated data to test the model, including demonstrating parameter recovery. To do this, we reorganize these three blocks into data (which now includes the assumed parameter values as data) and generated quantities blocks (which now includes the observations that will be generated).\n\n// Index and parameter values.\ndata {\n  int<lower = 1> N;    // Number of observations.\n  real mu;             // Mean of the regression.\n  real<lower = 0> tau; // Variance of the regression.\n}\n\n// Generate data according to the simple regression.\ngenerated quantities {\n  // Vector of observations.\n  vector[N] y;\n\n  // Generate data.\n  for (n in 1:N) {\n    y[n] = normal_rng(mu, tau);\n  }\n}\n\nWe’ll save this Stan script as generate_flat_data.stan. The for loop over y in the generated quantities block emphasizes the strong assumption of this flat or non-hierarchical model that a single mean mu and variance tau describe customer satisfaction for the entire population. If this doesn’t sit well with you, a hierarchical model will provide the cure. We’ll get there shortly.\nIn an R script, let’s load the necessary packages, allow Stan to use as many cores as we have available, allow for Stan to save compiled code, specify assumed parameter values, and generate data according to our simple flat regression by calling generate_flat_data.stan.\n\n# Load packages.\nlibrary(tidyverse)\nlibrary(rstan)\nlibrary(bayesplot)\nlibrary(tidybayes)\n\n# Set Stan options.\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\n# Specify data and parameter values.\nsim_values <- list(\n  N = 100, # Number of observations.\n  mu = 5,  # Mean of the regression.\n  tau = 1  # Variance of the regression.\n)\n\n# Generate data.\nsim_data <- stan(\n  file = here::here(\"Code\", \"generate_flat_data.stan\"),\n  data = sim_values,\n  iter = 1,\n  chains = 1,\n  seed = 42,\n  algorithm = \"Fixed_param\"\n)\n\nSAMPLING FOR MODEL 'generate_flat_data' NOW (CHAIN 1).\nChain 1: Iteration: 1 / 1 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0 seconds (Warm-up)\nChain 1:                0.000127 seconds (Sampling)\nChain 1:                0.000127 seconds (Total)\nChain 1: \nStan provides verbose output noting progress as well as any warnings. Everything looks fine here, so we can go ahead and extract our simulated data.\n\n# Extract simulated data.\nsim_y <- extract(sim_data)$y\n\nTo test our model, we simply specify the simulated data as a list of data to be used as an input, call the regression model flat_regression.stan from R, and Stan does all the heavy lifting for us.\n\n# Specify data.\ndata <- list(\n  N = length(sim_y),   # Number of observations.\n  y = as.vector(sim_y) # Vector of observations.\n)\n\n# Calibrate the model.\nfit <- stan(\n  file = here::here(\"Code\", \"flat_regression.stan\"),\n  data = data,\n  seed = 42\n)\n\nSAMPLING FOR MODEL 'flat_regression' NOW (CHAIN 4).\nChain 4: Gradient evaluation took 1.2e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.031208 seconds (Warm-up)\nChain 4:                0.024732 seconds (Sampling)\nChain 4:                0.05594 seconds (Total)\nChain 4: \nStan takes the three required blocks in flat_regression.stan and creates a Hamiltonian Monte Carlo sampler to produce draws from the posterior. The output tracks progress across many iterations (2000 by default) and chains (4 by default) along with any problems. Here we have printed the output for the fourth chain only and no problems have been identified. We can also check the trace plots.\n\n# Check trace plots.\nfit %>%\n  mcmc_trace(\n    pars = c(\"mu\", \"tau\"),\n    n_warmup = 500,\n    facet_args = list(nrow = 2, labeller = label_parsed)\n  )\n\n\nWe have good mixing and clear convergence across all chains for both of our model parameters.\nFinally, we can demonstrate parameter recovery by plotting the marginal posteriors for each of our parameters to confirm that the assumed parameter values used when generating data are within or near reasonable credible intervals.\n\n# Recover parameter values.\npar_values <- tibble(\n  .variable = c(\"mu\", \"tau\"),\n  values = c(sim_values$mu, sim_values$tau),\n)\n\nfit %>%\n  gather_draws(mu, tau) %>%\n  ggplot(aes(x = .value, y = .variable)) +\n  geom_halfeyeh(.width = .95) +\n  geom_vline(aes(xintercept = values), par_values, color = \"red\") +\n  facet_wrap(\n    ~ .variable,\n    nrow = 2,\n    scales = \"free\"\n  )\n\n\nThe assumed parameter values (in red) have been recovered by the model (they are in or nearly within a customary 95% credible interval)! In summary, we’ve specified a simple flat or non-hierarchical regression model in Stan, generated data according to that model, and used the generated data to demonstrate that the model is working as intended."
  },
  {
    "objectID": "posts/stan-hierarchical/index.html#simple-hierarchical-regression",
    "href": "posts/stan-hierarchical/index.html#simple-hierarchical-regression",
    "title": "Hierarchical models in Stan",
    "section": "Simple hierarchical regression",
    "text": "Simple hierarchical regression\nSo why use a hierarchical model (also known as a multilevel or nested multilevel model)? The most straightforward reason is that sinking feeling we have about the assumption that a single set of parameters will effectively describe the entire population. In terms of customer satisfaction, we know there is heterogeneity across consumers – not everyone behaves or thinks the same. At the very least, we can think about satisfaction being different for different customer groups or segments.\nSo what does a simple hierarchical regression look like in Stan?\n\n// Index values and observations.\ndata {\n  int<lower = 1> N;               // Number of observations.\n  int<lower = 1> K;               // Number of groups.\n  vector[N] y;                    // Vector of observations.\n  int<lower = 1, upper = K> g[N]; // Vector of group assignments.\n}\n\n// Parameters and hyperparameters.\nparameters {\n  real mu;                        // Mean of the population model.\n  real<lower = 0> tau;            // Variance of the population model.\n  vector[K] beta;                 // Vector of group intercepts.\n  real<lower = 0> sigma;          // Variance of the likelihood.\n}\n\n// Hierarchical regression.\nmodel {\n  // Hyperpriors.\n  mu ~ normal(0, 5);\n  tau ~ normal(0, 5);\n\n  // Prior.\n  sigma ~ normal(0, 5);\n\n  // Population model and likelihood.\n  beta ~ normal(mu, tau);\n  for (n in 1:N) {\n    y[n] ~ normal(beta[g[n]], sigma);\n  }\n}\n\nIn the data block, we now have a vector g indicating which one of K groups each of our N individuals belong to. In the parameter block, we now have a K-dimensional vector of beta parameters, a separate mean for each of the K groups. In the model block we can see that the likelihood (now within a for loop) is still assumed normal, but now each individual’s observed overall satisfaction has a mean of beta specific to the group they belong to. We also have a population model beta ~ normal(mu, tau) that says these separate, group-level beta coefficients are themselves drawn from a population that is assumed normal with a mean mu and variance tau.\nThis is what is meant by a hierarchy: there are two levels to our model, the lower-level likelihood or observation model and the upper-level population model. Finally, our joint model is complete with a prior on the population model parameters (formally referred to as hyperpriors, since they are priors on a prior): normal hyperpriors on mu and tau (constrained to be positive). The likelihood variance, now called sigma, retains the same prior and positive constraint.\nWe can again translate this Stan script, hierarchical_regression_01.stan, into data and generated quantities blocks. Let’s call this script generate_hierarchical_data_01.stan and use Stan to generate data for us.\n\n// Index and hyperparameter values.\ndata {\n  int<lower = 1> N;               // Number of observations.\n  int<lower = 1> K;               // Number of groups.\n  int<lower = 1, upper = K> g[N]; // Vector of group assignments.\n  real mu;                        // Mean of the population model.\n  real<lower = 0> tau;            // Variance of the population model.\n  real<lower = 0> sigma;          // Variance of the likelihood.\n}\n\n// Generate data according to the hierarchical regression.\ngenerated quantities {\n  vector[N] y;                    // Vector of observations.\n  vector[K] beta;                 // Vector of group intercepts.\n\n  // Draw parameter values and generate data.\n  for (k in 1:K) {\n    beta[k] = normal_rng(mu, tau);\n  }\n  for (n in 1:N) {\n    y[n] = normal_rng(beta[g[n]], sigma);\n  }\n}\n\nNote that the data block now includes the assumed parameter values for the hyperpriors as data with beta being drawn alongside y in the generated quantities block. Let’s call this Stan script from R to generate data according to a hierarchical model.\n\n# Specify data and hyperparameter values.\nsim_values <- list(\n  N = 500,                            # Number of observations.\n  K = 5,                              # Number of groups.\n  g = sample(5, 500, replace = TRUE), # Vector of group assignments.\n  mu = 5,                             # Mean of the population model.\n  tau = 1,                            # Variance of the population model.\n  sigma = 1                           # Variance of the likelihood.\n)\n\n# Generate data.\nsim_data <- stan(\n  file = here::here(\"Code\", \"generate_hierarchical_data_01.stan\"),\n  data = sim_values,\n  iter = 1,\n  chains = 1,\n  seed = 42,\n  algorithm = \"Fixed_param\"\n)\n\n# Extract simulated data and group intercepts.\nsim_y <- extract(sim_data)$y\nsim_beta <- extract(sim_data)$beta\n\nWe can test our simple hierarchical model using our new simulated data to fit the hierarchical regression from R.\n\n# Specify data.\ndata <- list(\n  N = length(sim_y),    # Number of observations.\n  K = sim_values$K,     # Number of groups.\n  y = as.vector(sim_y), # Vector of observations.\n  g = sim_values$g      # Vector of group assignments.\n)\n\n# Calibrate the model.\nfit <- stan(\n  file = here::here(\"Code\", \"hierarchical_regression_01.stan\"),\n  data = data,\n  seed = 42\n)\n\nWarning messages:\n1: There were 6 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup \n2: Examine the pairs() plot to diagnose sampling problems\nOur sampler has identified a problem. We only have a simple hierarchical model and already our unique Hamiltonian Monte Carlo diagnostic, a divergent transition or divergence, has identified an issue. There is a good reason for these divergences. The short story is that hierarchical models have posteriors with geometry that is difficult to navigate. This is true independent of the sampler, but it’s quickly evident when using Hamiltonian Monte Carlo.\nTo address this issue, our first step (and one that is helpfully suggested in the warning message above) is to increase the adapt_delta, which functions like a step size as the sampler navigates the posterior. A larger adapt_delta equates to a smaller step size, meaning the sampler will be more careful as it navigates but it will also take longer. We find that adapt_delta = 0.99 (it only goes to 1) results in no divergences.\n\n# Calibrate the model.\nfit <- stan(\n  file = here::here(\"Code\", \"hierarchical_regression_01.stan\"),\n  data = data,\n  control = list(adapt_delta = 0.99),\n  seed = 42\n)\n\nWe can now check trace plots, this time for both the population hyperparameters and the group parameters.\n\n# Check trace plots.\nfit %>%\n  mcmc_trace(\n    pars = c(\"mu\", \"tau\", str_c(\"beta[\", 1:data$K, \"]\"), \"sigma\"),\n    n_warmup = 500,\n    facet_args = list(nrow = 5, labeller = label_parsed)\n  )\n\n\nFinally, we can demonstrate parameter recovery for both the population-level hyperparameters and the group-level parameters.\n\n# Recover hyperparameter and parameter values.\nhyper_par_values <- tibble(\n  .variable = c(\"mu\", \"tau\", \"sigma\"),\n  values = c(sim_values$mu, sim_values$tau, sim_values$sigma),\n)\n\nbeta_values <- tibble(\n  n = 1:data$K,\n  beta = as.vector(sim_beta)\n)\n\nfit %>%\n  gather_draws(mu, tau, sigma) %>%\n  ggplot(aes(x = .value, y = .variable)) +\n  geom_halfeyeh(.width = .95) +\n  geom_vline(aes(xintercept = values), hyper_par_values, color = \"red\") +\n  facet_wrap(\n    ~ .variable,\n    nrow = 3,\n    scales = \"free\"\n  )\n\n\n\nfit %>%\n  spread_draws(beta[n]) %>%\n  ggplot(aes(x = beta, y = n)) +\n  geom_halfeyeh(.width = .95) +\n  geom_vline(aes(xintercept = beta), beta_values, color = \"red\") +\n  facet_wrap(\n    ~ n,\n    nrow = 3,\n    scales = \"free\"\n  )\n\n\nBoth the assumed hyperparameter and parameter values have been recovered by the model! To summarize, we have now specified a simple hierarchical regression model in Stan, generated data according to that model, and used the generated data to demonstrate that the model is working."
  },
  {
    "objectID": "posts/stan-hierarchical/index.html#multiple-hierarchical-regression",
    "href": "posts/stan-hierarchical/index.html#multiple-hierarchical-regression",
    "title": "Hierarchical models in Stan",
    "section": "Multiple hierarchical regression",
    "text": "Multiple hierarchical regression\nIn this final section, let’s more fully embrace what it means to have a hierarchical model. With both a likelihood or observation model and a population model, we can include both observation-level and population-level covariates.\nExciting, right? Let’s get started.\n\n// Index values, observations, and covariates.\ndata {\n  int<lower = 1> N;               // Number of observations.\n  int<lower = 1> K;               // Number of groups.\n  int<lower = 1> I;               // Number of observation-level covariates.\n  int<lower = 1> J;               // Number of population-level covariates.\n\n  vector[N] y;                    // Vector of observations.\n  int<lower = 1, upper = K> g[N]; // Vector of group assignments.\n  matrix[N, I] X;                 // Matrix of observation-level covariates.\n  matrix[K, J] Z;                 // Matrix of population-level covariates.\n}\n\n// Parameters and hyperparameters.\nparameters {\n  matrix[J, I] Gamma;             // Matrix of population-level coefficients.\n  real<lower = 0> tau;            // Variance of the population model.\n  matrix[K, I] Beta;              // Matrix of observation-level coefficients.\n  real<lower = 0> sigma;          // Variance of the likelihood.\n}\n\n// Hierarchical regression.\nmodel {\n  // Hyperpriors.\n  for (j in 1:J) {\n    Gamma[j,] ~ normal(0, 5);\n  }\n  tau ~ normal(0, 5);\n\n  // Prior.\n  sigma ~ normal(0, 5);\n\n  // Population model and likelihood.\n  for (k in 1:K) {\n    Beta[k,] ~ normal(Z[k,] * Gamma, tau);\n  }\n  for (n in 1:N) {\n    y[n] ~ normal(X[n,] * Beta[g[n],]', sigma);\n  }\n}\n\nIn the data block, we now have an index I specifying the number of observation-level covariates and an index J specifying the number of population-level covariates along with a corresponding N x I matrix X that contains the observation-level covariates (including an intercept) and a K x J matrix Z that contains the population-level covariates (again, including an intercept).\nIn the parameters block, Beta is now a K x I matrix of coefficients rather than a vector and Gamma, a J x I matrix of population-level coefficients, takes the place of mu. The model block also changes accordingly, with the population model in a for loop and including Z[k,] * Gamma in place of mu and the likelihood including X[n,] * Beta[g[n],]' in place of beta[g[n]]. Note, however, that we are still assuming that the population model is distributed normal with a common variance tau. We’ll come back to this at the end.\nWe can again translate this Stan script, hierarchical_regression_02.stan, into data and generated quantities blocks. Let’s call this script generate_hierarchical_data_02.stan and use Stan to generate data for us.\n\n// Index values, covariates, and hyperparameter values.\ndata {\n  int<lower = 1> N;               // Number of observations.\n  int<lower = 1> K;               // Number of groups.\n  int<lower = 1> I;               // Number of observation-level covariates.\n  int<lower = 1> J;               // Number of population-level covariates.\n\n  int<lower = 1, upper = K> g[N]; // Vector of group assignments.\n  matrix[N, I] X;                 // Matrix of observation-level covariates.\n  matrix[K, J] Z;                 // Matrix of population-level covariates.\n  real<lower = 0> tau;            // Variance of the population model.\n  real<lower = 0> sigma;          // Variance of the likelihood.\n}\n\n// Generate data according to the hierarchical regression.\ngenerated quantities {\n  vector[N] y;                    // Vector of observations.\n  matrix[J, I] Gamma;             // Matrix of population-level coefficients.\n  matrix[K, I] Beta;              // Matrix of group-level coefficients.\n\n  // Draw parameter values and generate data.\n  for (j in 1:J) {\n    for (i in 1:I) {\n      Gamma[j, i] = normal_rng(0, 5);\n    }\n  }\n  for (k in 1:K) {\n    for (i in 1:I) {\n      Beta[k, i] = normal_rng(Z[k,] * Gamma[,i], tau);\n    }\n  }\n  for (n in 1:N) {\n    y[n] = normal_rng(X[n,] * Beta[g[n],]', sigma);\n  }\n}\n\nNote that the data block now includes the X and Z matrices, the observation-level and population-level covariates, respectively. We could also generate these matrices, each of which includes an intercept as the first column, in the generated quantities block instead of generating them in R. Also note that in the generated quantities block we have an additional for loop for simulating Beta and Gamma since we are working with matrices of coefficients instead of vectors. Let’s generate data by calling this Stan script from R.\n\n# Specify data and hyperparameter values.\nsim_values <- list(\n  N = 500,                            # Number of observations.\n  K = 5,                              # Number of groups.\n  I = 7,                              # Number of observation-level covariates.\n  J = 3,                              # Number of population-level covariates.\n\n  # Matrix of observation-level covariates.\n  X = cbind(\n    rep(1, 500),\n    matrix(runif(500 * (7 - 1), min = 1, max = 10), nrow = 500)\n  ),\n\n  # Matrix of population-level covariates.\n  Z = cbind(\n    rep(1, 5),\n    matrix(runif(5 * (3 - 1), min = 2, max = 5), nrow = 5)\n  ),\n\n  g = sample(5, 500, replace = TRUE), # Vector of group assignments.\n  tau = 1,                            # Variance of the population model.\n  sigma = 1                           # Variance of the likelihood.\n)\n\n# Generate data.\nsim_data <- stan(\n  file = here::here(\"Code\", \"generate_hierarchical_data_02.stan\"),\n  data = sim_values,\n  iter = 1,\n  chains = 1,\n  seed = 42,\n  algorithm = \"Fixed_param\"\n)\n\n# Extract simulated data and group intercepts.\nsim_y <- extract(sim_data)$y\nsim_Gamma <- extract(sim_data)$Gamma\nsim_Beta <- extract(sim_data)$Beta\n\nLet’s fit the model and recover parameters.\n\n# Specify data.\ndata <- list(\n  N = sim_values$N,     # Number of observations.\n  K = sim_values$K,     # Number of groups.\n  I = sim_values$I,     # Number of observation-level covariates.\n  J = sim_values$J,     # Number of population-level covariates.\n  y = as.vector(sim_y), # Vector of observations.\n  g = sim_values$g,     # Vector of group assignments.\n  X = sim_values$X,     # Matrix of observation-level covariates.\n  Z = sim_values$Z      # Matrix of population-level covariates.\n)\n\n# Calibrate the model.\nfit <- stan(\n  file = here::here(\"Code\", \"hierarchical_regression_02.stan\"),\n  data = data,\n  control = list(adapt_delta = 0.99),\n  seed = 42\n)\n\nNote that even though we’ve preemptively set adapt_delta = 0.99 given the previous model’s performance, it will be easy to run into problems as we add more complexity to the posterior. We’ll touch on a common solution at the end. For now the model has finished calibration without any warnings.\nLet’s evaluate trace plots.\n\n# Check population model trace plots.\ngamma_string <- str_c(\"Gamma[\", 1:data$J, \",\", 1, \"]\")\nfor (i in 2:data$I) {\n  gamma_temp <- str_c(\"Gamma[\", 1:data$J, \",\", i, \"]\")\n  gamma_string <- c(gamma_string, gamma_temp)\n}\nfit %>%\n  mcmc_trace(\n    pars = c(gamma_string, \"tau\"),\n    n_warmup = 500,\n    facet_args = list(\n      nrow = ceiling(length(c(gamma_string, \"tau\")) / 4),\n      ncol = 4,\n      labeller = label_parsed\n    )\n  )\n\n\n\n# Check observation model trace plots.\nbeta_string <- str_c(\"Beta[\", 1:data$K, \",\", 1, \"]\")\nfor (i in 2:data$I) {\n  beta_temp <- str_c(\"Beta[\", 1:data$K, \",\", i, \"]\")\n  beta_string <- c(beta_string, beta_temp)\n}\nfit %>%\n  mcmc_trace(\n    pars = c(beta_string, \"sigma\"),\n    n_warmup = 500,\n    facet_args = list(\n      nrow = ceiling(length(c(beta_string, \"sigma\")) / 4),\n      ncol = 4,\n      labeller = label_parsed\n    )\n  )\n\n\nOnce again, we have good mixing and clear convergence across all chains, now to demonstrate parameter recovery for our many model parameters.\n\n# Recover Gamma values.\ngamma_values <- tibble(\n  j = sort(rep(1:(data$J), data$I)),\n  i = rep(1:(data$I), data$J),\n  .variable = str_c(\"Gamma\", \"_\", j, \"_\", i),\n  values = as.vector(t(matrix(sim_Gamma, ncol = data$I)))\n) %>%\n  select(.variable, values)\n\nfit %>%\n  gather_draws(Gamma[j, i]) %>%\n  unite(.variable, .variable, j, i) %>%\n  ggplot(aes(x = .value, y = .variable)) +\n  geom_halfeyeh(.width = .95) +\n  geom_vline(aes(xintercept = values), gamma_values, color = \"red\") +\n  facet_wrap(\n    ~ .variable,\n    nrow = data$J,\n    ncol = data$I,\n    scales = \"free\"\n  )\n\n\n\n# Recover Beta values.\nbeta_values <- tibble(\n  n = sort(rep(1:(data$K), data$I)),\n  i = rep(1:(data$I), data$K),\n  .variable = str_c(\"Beta\", \"_\", n, \"_\", i),\n  values = as.vector(t(matrix(sim_Beta, ncol = data$I)))\n) %>%\n  select(.variable, values)\n\nfit %>%\n  gather_draws(Beta[n, i]) %>%\n  unite(.variable, .variable, n, i) %>%\n  ggplot(aes(x = .value, y = .variable)) +\n  geom_halfeyeh(.width = .95) +\n  geom_vline(aes(xintercept = values), beta_values, color = \"red\") +\n  facet_wrap(\n    ~ .variable,\n    nrow = data$K,\n    ncol = data$I,\n    scales = \"free\"\n  )\n\n\n\n# Recover tau and sigma values.\nhyper_par_values <- tibble(\n  .variable = c(\"tau\", \"sigma\"),\n  values = c(sim_values$tau, sim_values$sigma),\n)\n\nfit %>%\n  gather_draws(tau, sigma) %>%\n  ggplot(aes(x = .value, y = .variable)) +\n  geom_halfeyeh(.width = .95) +\n  geom_vline(aes(xintercept = values), hyper_par_values, color = \"red\") +\n  facet_wrap(\n    ~ .variable,\n    nrow = 2,\n    scales = \"free\"\n  )\n\n\nAll the assumed hyperparameter and parameter values have been recovered by the model! To summarize, we have now specified a multiple hierarchical regression model in Stan – with covariates at both the observation and population levels – generated data according to that model, and used the generated data to demonstrate that the model is working."
  },
  {
    "objectID": "posts/stan-hierarchical/index.html#final-thoughts",
    "href": "posts/stan-hierarchical/index.html#final-thoughts",
    "title": "Hierarchical models in Stan",
    "section": "Final thoughts",
    "text": "Final thoughts\nWhile we haven’t touched on a complete Bayesian inference workflow, including evaluating prior and likelihood specifications using prior and posterior predictive checks, hopefully this provides a helpful starting point for using hierarchical models in Stan.\nNote that we assumed throughout that the population model has a common variance. Relaxing this assumption requires that the population model be distributed multivariate normal with an accompanying covariance matrix, a far more flexible population model specification but one that adds enough complexity to the posterior geometry to require a non-centered parameterization. We’ll follow up on this topic in its own blog post.\nHierarchical models provide partial information pooling, a middle-ground between no information pooling (i.e., separate flat or non-hierarchical models for each group), which may not be possible, and complete information pooling (i.e., a single flat or non-hierarchical model), which provides limited inference. They also allow for covariates and parameter estimation at both the observation and population levels. To summarize, hierarchical models allow us to appropriately model heterogeneity – differences across groups. Given the prevalence of heterogeneity, hierarchical models should be our default approach in most applications and Stan removes the barriers to entry for the required Bayesian computation.\n\n\nMarc Dotson\nMarc is an assistant professor of marketing at the BYU Marriott School of Business. He graduated with an MSc from The London School of Economics and Political Science in 2009 and a PhD from The Ohio State University in 2016. His research interests include Bayesian inference, predictive modeling, consumer preference heterogeneity, and unstructured data. Marc teaches marketing analytics. You can find him on Twitter and GitHub."
  },
  {
    "objectID": "posts/choice-models/index.html",
    "href": "posts/choice-models/index.html",
    "title": "Comparing choice model parameterizations",
    "section": "",
    "text": "Choice models are common in marketing and other applications where researchers are interested in understanding both the drivers and trade-offs of choice. Since choice is typically manifest as a non-binary discrete outcome and we care about modeling consumer heterogeneity, a hierarchical Bayesian multinomial logit model is our default specification.\nIn marketing, choice models are often employed in conjunction with conjoint experiments, a survey-based approach to eliciting preferences where respondents choose from sets of hypothetical product alternatives. Because the conjoint experiment produces repeat observations at the respondent level, the groups in the hierarchical model are the respondents themselves.\nIn this post we will specify a choice model and seek to answer the following question: Does it matter if we use a centered or non-centered parameterization for a hierarchical multinomial logit model? We have previously explored using a non-centered parameterization for hierarchical regression and even argued that a non-centered parameterization should be our default approach in most applications. However, choice models used in practice employ a centered parameterization almost exclusively.\nWe will specify and compare performance of a centered parameterization and a non-centered parameterization of a hierarchical multinomial logit model estimated using Hamilton Monte Carlo (HMC) via Stan. Stan automates the required computation for many models and HMC is particularly verbose regarding model diagnostics. I am indebted to early tutorials on this topic by Elea Feit and Kevin Van Horn and Jim Savage."
  },
  {
    "objectID": "posts/choice-models/index.html#centered-parameterization",
    "href": "posts/choice-models/index.html#centered-parameterization",
    "title": "Comparing choice model parameterizations",
    "section": "Centered parameterization",
    "text": "Centered parameterization\nLet’s start with a centered parameterization of the hierarchical multinomial logit.\n\n// Index values, hyperprior values, observations, and covariates.\ndata {\n  int<lower = 1> R;                  // Number of respondents.\n  int<lower = 1> S;                  // Number of choice tasks.\n  int<lower = 2> A;                  // Number of choice alternatives.\n  int<lower = 1> I;                  // Number of observation-level covariates.\n  int<lower = 1> J;                  // Number of population-level covariates.\n\n  real Gamma_mean;                   // Mean of population-level means.\n  real<lower=0> Gamma_scale;         // Scale of population-level means.\n  real<lower=0> Omega_shape;         // Shape of population-level scale.\n  real tau_mean;                     // Mean of population-level scale.\n  real<lower=0> tau_scale;           // Scale of population-level scale.\n\n  int<lower = 1, upper = A> Y[R, S]; // Matrix of observations.\n  matrix[A, I] X[R, S];              // Array of observation-level covariates.\n  matrix[R, J] Z;                    // Matrix of population-level covariates.\n}\n\n// Parameters and hyperparameters.\nparameters {\n  matrix[J, I] Gamma;                // Matrix of population-level hyperparameters.\n  corr_matrix[I] Omega;              // Population model correlation matrix hyperparameters.\n  vector<lower = 0>[I] tau;          // Population model vector of scale hyperparameters.\n  matrix[R, I] Beta;                 // Matrix of observation-level parameters.\n}\n\n// Hierarchical multinomial logit.\nmodel {\n  // Hyperpriors.\n  to_vector(Gamma) ~ normal(Gamma_mean, Gamma_scale);\n  Omega ~ lkj_corr(Omega_shape);\n  tau ~ normal(tau_mean, tau_scale);\n\n  // Population model and likelihood.\n  for (r in 1:R) {\n    Beta[r,] ~ multi_normal(Z[r,] * Gamma, quad_form_diag(Omega, tau));\n    for (s in 1:S) {\n      Y[r, s] ~ categorical_logit(X[r, s] * Beta[r,]');\n    }\n  }\n}\n\nWe’ll save this Stan script as hmnl_centered.stan. In the data block, we have R respondents that each go through S choice tasks where each choice task has A choice alternatives to choose from. Thus there are R * S total observations where each observation takes on a value from 1 to A, the chosen alternative. At the observation level, the choice alternatives are defined by I covariates. These covariates are the attributes of the choice alternatives under consideration. At the population or group level, the respondents are defined by J covariates. These covariates are used to explain preference heterogeneity across respondents in the population and improve our ability to predict preferences.\nThe data block also includes the hyperprior values. These define our hyperpriors on the hyperparameters in the population-level model and can be more easily evaluated when specified as part of the data block rather than hard-coded in the model block. Finally, the observations are stored as a matrix Y where each of the R respondents’ S choices are stored as a row while the observation-level covariates X is an R x S array where each element is the A x I matrix of covariates specific to that respondent and choice task.\nThe parameters block includes the population-level hyperparameters Gamma or the coefficients associated with the population-level covariates Z, the population-level hyperparameters Omega and tau which are the accompanying correlation matrix and scale of the population-level model, and the observation-level parameters Beta. Again, because a conjoint experiment produces repeat observations at the respondent level, we get a set of these Beta parameters for each respondent. These parameters are often referred to as part-worth utilities or preference parameters in the conjoint literature as they describe the preferences each respondent has for each of the attributes that define the choice alternatives. Thus the hyperparameters are estimates of population preference heterogeneity.\nThe model block includes the specification of hyperpriors, using the hyperprior values specified in the data block, and the hierarchy of models: the population model on the Beta parameters and the observation model or likelihood on the observed choices Y. We use the helper function to_vector() to easily use a normal() hyperprior on the Gamma matrix. By decomposing the covariance matrix of the multivariate normal population model into a correlation matrix and a scale vector means we can use an LKJ hyperprior on the correlation matrix Omega and a normal hyperprior on the scale vector tau. We then reform the covariance matrix using quad_form_diag() as part of the multivariate population model. Note that categorical_logit() is the multinomial logit likelihood and that it must be used within nested for loops since it isn’t vectorized. Also note that since Beta is a matrix where each row is a vector of respondent-specific parameters, the vector output of multi_normal() is transposed as part of the likelihood.\nSince Bayesian models are generative, we can translate this Stan script into data and generated quantities blocks and use Stan to generate data for us.\n\n// Index values, hyperprior values, and covariates.\ndata {\n  int<lower = 1> R;                  // Number of respondents.\n  int<lower = 1> S;                  // Number of choice tasks.\n  int<lower = 2> A;                  // Number of choice alternatives.\n  int<lower = 1> I;                  // Number of observation-level covariates.\n  int<lower = 1> J;                  // Number of population-level covariates.\n\n  real Gamma_mean;                   // Mean of population-level means.\n  real<lower=0> Gamma_scale;         // Scale of population-level means.\n  real<lower=0> Omega_shape;         // Shape of population-level scale.\n  real tau_df;                       // Degrees of freedom of population-level scale.\n\n  matrix[A, I] X[R, S];              // Array of observation-level covariates.\n  matrix[R, J] Z;                    // Matrix of population-level covariates.\n}\n\n// Generate data according to the hierarchical multinomial logit.\ngenerated quantities {\n  int<lower = 1, upper = A> Y[R, S]; // Matrix of observations.\n  matrix[J, I] Gamma;                // Matrix of population-level hyperparameters.\n  corr_matrix[I] Omega;              // Population model correlation matrix hyperparameters.\n  vector[I] tau;                     // Population model vector of scale hyperparameters.\n  matrix[R, I] Beta;                 // Matrix of observation-level parameters.\n\n  // Draw parameter values and generate data.\n  for (j in 1:J) {\n    for (i in 1:I) {\n      Gamma[j, i] = normal_rng(Gamma_mean, Gamma_scale);\n    }\n  }\n  for (i in 1:I) {\n    tau[i] = chi_square_rng(tau_df);\n  }\n  Omega = lkj_corr_rng(I, Omega_shape);\n  for (r in 1:R) {\n    Beta[r,] = multi_normal_rng(Z[r,] * Gamma, quad_form_diag(Omega, tau))';\n    for (s in 1:S) {\n      Y[r, s] = categorical_logit_rng(X[r, s] * Beta[r,]');\n    }\n  }\n}\n\nWe’ll save this Stan script as generate_data.stan. While providing the covariate matrices X and Z in data, largely because this is more straightforward to generate in R, we are generating hyperparameter and parameters values in generated quantities. This includes using the LKJ hyperprior to generate the values of the correlation matrix Omega. However, instead of using a normal hyperprior to generate the scale vector hyperparameters tau, we use a Chi-square to ensure positive values. Once again, since Beta is a matrix where each row is a vector of respondent parameters, the vector output of multi_normal_rng() is transposed. Note that while sampling statements like ~ normal() from the model block of the estimation code are vectorized, none of the corresponding RNG statements like = normal_rng() are, hence all of the additional for loops in the corresponding generated quantities block in generate_data.stan.\nIn an R script, let’s load the necessary packages, allow Stan to use as many cores as we have available and save compiled code, and specify the structure of the data and the hyperprior values.\n\n# Load packages.\nlibrary(tidyverse)\nlibrary(rstan)\nlibrary(bayesplot)\nlibrary(tidybayes)\n\n# Set Stan options.\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\n# Specify data and hyperprior values.\nsim_values <- list(\n  R = 500,           # Number of respondents.\n  S = 10,            # Number of choice tasks.\n  A = 4,             # Number of choice alternatives.\n  L = c(3, 4, 5),    # Number of levels in each discrete attribute.\n  I = 12,            # Number of estimable attribute levels, including the brand intercept.\n  J = 3,             # Number of population-level covariates.\n\n  Gamma_mean = 0,    # Mean of population-level means.\n  Gamma_scale = 5,   # Scale of population-level means.\n  Omega_shape = 2,   # Shape of population-level scale.\n  tau_df = 2         # Degrees of freedom of population-level scale.\n)\n\nHere sim_values is close to the data input for using generate_data.stan. These are typical dimensions for a conjoint experiment with R = 500 respondents, S = 10 choice tasks, A = 4 choice alternatives, and I = 12 estimable attribute levels. One important difference from the data block is that we’ve included an additional variable L that specifies the number of levels in each discrete attribute. In conjoint experiments, most of the attributes are discrete. We want to keep track of which of the simulated attributes are discrete, by design appearing before any continuous attributes, so we can easily impose dummy coding by dropping the first level of each attribute. The first attribute is typically the brand of the product and rather than including an intercept, we will allow for all brand levels to be present, thus creating a “brand intercept” where everything for a given product alternative not explained by the associated attributes is included in the brand.\nThus I, the number of estimable attribute levels, will be the number of brand levels plus the number of levels for the remaining attribute levels minus the hold-out level for each attribute plus the number of any continuous attributes. Since I = 12 and there are three brands, L[1] = 3, we have specified two continuous attributes, as follows.\n\n# Array of observation-level covariates.\nX <- array(\n  NA,\n  dim = c(sim_values$R, sim_values$S, sim_values$A, sim_values$I)\n)\nfor (r in 1:sim_values$R) {\n  for (s in 1:sim_values$S) {\n    # Discrete predictors.\n    X_s <- NULL\n    for (l in 1:length(sim_values$L)) {\n      X_l <- NULL\n      for (a in 1:sim_values$A) {\n        X_a <- matrix(0, nrow = 1, ncol = sim_values$L[l])\n        X_a[1, sample(seq(1, sim_values$L[l]), 1)] <- 1\n        if (l == 1) X_l <- rbind(X_l, X_a)\n        if (l != 1) X_l <- rbind(X_l, X_a[, -1])\n      }\n      X_s <- cbind(X_s, X_l)\n    }\n    # Continuous predictors.\n    L_n <- sim_values$I - (sum(sim_values$L) - length(sim_values$L) + 1)\n    if(L_n != 0) {\n      X_s <- cbind(X_s, matrix(rnorm(sim_values$A * L_n), ncol = L_n))\n    }\n    X[r, s, , ] <- X_s\n  }\n}\nsim_values$X <- X\n\nWe can check the design matrix for the first respondent to ensure we have 12 estimable attribute levels, including the brand intercept and two continuous variables.\n\nsim_values$X[1,1,,]\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]      [,11]      [,12]\n[1,]    1    0    0    0    0    1    1    0    0     0  1.4235293 -1.1998511\n[2,]    1    0    0    0    0    1    0    0    0     0 -0.7737283  0.3324931\n[3,]    0    0    1    1    0    0    0    0    0     0 -0.2326687  1.4514409\n[4,]    1    0    0    0    1    0    0    0    0     1 -1.0825479  1.1937547\nNow we can generate the covariate matrix Z and then generate the remainder of our data according to the hierarchical multinomial logit by calling generate_data.stan.\n\n# Matrix of population-level covariates.\nZ <- cbind(\n  rep(1, sim_values$R),\n  matrix(\n    runif(sim_values$R * (sim_values$J - 1), min = 2, max = 5),\n    nrow = sim_values$R\n  )\n)\nsim_values$Z <- Z\n\n# Generate data.\nsim_data <- stan(\n  file = here::here(\"Code\", \"generate_data.stan\"),\n  data = sim_values,\n  iter = 1,\n  chains = 1,\n  seed = 42,\n  algorithm = \"Fixed_param\"\n)\n\n# Extract simulated data and parameters.\nsim_Y <- extract(sim_data)$Y[1,,]\nsim_Gamma <- extract(sim_data)$Gamma[1,,]\nsim_Omega <- extract(sim_data)$Omega[1,,]\nsim_tau <- extract(sim_data)$tau[1,]\nsim_Beta <- extract(sim_data)$Beta[1,,]\n\nNow let’s estimate our model by calling hmnl_centered.stan.\n\ndata <- list(\n  R = sim_values$R,    # Number of respondents.\n  S = sim_values$S,    # Number of choice tasks.\n  A = sim_values$A,    # Number of choice alternatives.\n  I = sim_values$I,    # Number of observation-level covariates.\n  J = sim_values$J,    # Number of population-level covariates.\n\n  Gamma_mean = 0,      # Mean of population-level means.\n  Gamma_scale = 5,     # Scale of population-level means.\n  Omega_shape = 2,     # Shape of population-level scale.\n  tau_mean = 0,        # Mean of population-level scale.\n  tau_scale = 5,       # Scale of population-level scale.\n\n  Y = sim_Y,           # Matrix of observations.\n  X = sim_values$X,    # Array of observation-level covariates.\n  Z = sim_values$Z     # Matrix of population-level covariates.\n)\n\nfit_centered <- stan(\n  file = here::here(\"Code\", \"hmnl_centered.stan\"),\n  data = data,\n  iter = 4000,\n  thin = 2,\n  seed = 42\n)\n\nChain 4:  Elapsed Time: 21419.5 seconds (Warm-up)\nChain 4:                23963 seconds (Sampling)\nChain 4:                45382.5 seconds (Total)\nChain 4: \nWarning messages:\n1: There were 311 divergent transitions after warmup. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them. \n2: There were 3689 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See\nhttp://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded \n3: There were 4 chains where the estimated Bayesian Fraction of Missing Information was low. See\nhttp://mc-stan.org/misc/warnings.html#bfmi-low \n4: Examine the pairs() plot to diagnose sampling problems\n \n5: The largest R-hat is NA, indicating chains have not mixed.\nRunning the chains for more iterations may help. See\nhttp://mc-stan.org/misc/warnings.html#r-hat \n6: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttp://mc-stan.org/misc/warnings.html#bulk-ess \n7: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttp://mc-stan.org/misc/warnings.html#tail-ess \nEstimating the model gives us more than 300 divergent transitions. Recall that a divergent transition or divergence is a unique HMC diagnostic that identifies problems navigating the posterior distribution. These difficulties with posterior geometry are true regardless of the sampler, but HMC makes the issue more transparent. We continue to see divergences even if we set adapt_delta = 0.99 as we’ve done with hierarchical models previously. There are also warnings suggesting we run the chains for longer. Even up to iter = 10000, five times the default 2000 draws, we see the same problems.\nRegardless, the fact that we run into divergences when using simulated data with typical conjoint experiment dimensions says that in order to produce a posterior geometry that can be navigated, we need to reparameterize our model."
  },
  {
    "objectID": "posts/choice-models/index.html#non-centered-parameterization",
    "href": "posts/choice-models/index.html#non-centered-parameterization",
    "title": "Comparing choice model parameterizations",
    "section": "Non-centered parameterization",
    "text": "Non-centered parameterization\nA non-centered parameterization re-expresses the population model and likelihood for a hierarchical model by including an intermediate, deterministic transformation on a set of the hyperparameters. The benefit is that difficult dependencies between the layers in the hierarchy are broken, producing a simpler posterior geometry. See a previous post for a walkthrough of the re-parameterization using a simplified population model as part of a hierarchical linear model.\nLet’s see a non-centered parameterization of the hierarchical multinomial logit.\n\n// Index values, hyperprior values, observations, and covariates.\ndata {\n  int<lower = 1> R;                  // Number of respondents.\n  int<lower = 1> S;                  // Number of choice tasks.\n  int<lower = 2> A;                  // Number of choice alternatives.\n  int<lower = 1> I;                  // Number of observation-level covariates.\n  int<lower = 1> J;                  // Number of population-level covariates.\n\n  real Gamma_mean;                   // Mean of population-level means.\n  real<lower=0> Gamma_scale;         // Scale of population-level means.\n  real<lower=0> Omega_shape;         // Shape of population-level scale.\n  real tau_mean;                     // Mean of population-level scale.\n  real<lower=0> tau_scale;           // Scale of population-level scale.\n\n  int<lower = 1, upper = A> Y[R, S]; // Matrix of observations.\n  matrix[A, I] X[R, S];              // Array of observation-level covariates.\n  matrix[R, J] Z;                    // Matrix of population-level covariates.\n}\n\n// Parameters and hyperparameters.\nparameters {\n  matrix[J, I] Gamma;                // Matrix of population-level hyperparameters.\n  corr_matrix[I] Omega;              // Population model correlation matrix hyperparameters.\n  vector<lower = 0>[I] tau;          // Population model vector of scale hyperparameters.\n  matrix[R, I] Delta;                // Matrix of non-centered observation-level parameters.\n}\n\n// Deterministic transformation.\ntransformed parameters {\n  // Matrix of centered observation-level parameters.\n  matrix[R, I] Beta;\n\n  // Non-centered parameterization.\n  for (r in 1:R) {\n    Beta[r,] = Z[r,] * Gamma + Delta[r,] * quad_form_diag(Omega, tau);\n  }\n}\n\n// Hierarchical multinomial logit model.\nmodel {\n  // Hyperpriors.\n  to_vector(Gamma) ~ normal(Gamma_mean, Gamma_scale);\n  Omega ~ lkj_corr(Omega_shape);\n  tau ~ normal(tau_mean, tau_scale);\n\n  // Non-centered population model and likelihood.\n  for (r in 1:R) {\n    Delta[r,] ~ normal(0, 1);\n    for (s in 1:S) {\n      Y[r, s] ~ categorical_logit(X[r, s] * Beta[r,]');\n    }\n  }\n}\n\nWe’ll save this Stan script as hmnl_noncentered.stan. We now have the Delta hyperparameters in place of Beta in the parameters block, a transformed parameters block that introduces the Beta hyperparameters and imposes the deterministic non-centered parameterization, and now a standard normal population model in the model block.\nThe generated data doesn’t change, so we can go ahead and estimate our model by calling hmnl_noncentered.stan.\n\ndata <- list(\n  R = sim_values$R,    # Number of respondents.\n  S = sim_values$S,    # Number of choice tasks.\n  A = sim_values$A,    # Number of choice alternatives.\n  I = sim_values$I,    # Number of observation-level covariates.\n  J = sim_values$J,    # Number of population-level covariates.\n\n  Gamma_mean = 0,      # Mean of population-level means.\n  Gamma_scale = 5,     # Scale of population-level means.\n  Omega_shape = 2,     # Shape of population-level scale.\n  tau_mean = 0,        # Mean of population-level scale.\n  tau_scale = 5,       # Scale of population-level scale.\n\n  Y = sim_Y,           # Matrix of observations.\n  X = sim_values$X,    # Array of observation-level covariates.\n  Z = sim_values$Z     # Matrix of population-level covariates.\n)\n\n# Calibrate the model.\nfit_noncentered <- stan(\n  file = here::here(\"Code\", \"hmnl_noncentered.stan\"),\n  data = data,\n  iter = 4000,\n  thin = 2,\n  seed = 42\n)\n\nThe model runs without divergences! There also aren’t any diagnostic warnings, beyond an initial suggestion for longer chains, hence we added iter = 4000. For convenience, we also thin the posterior draws with thin = 2 to reduce the size of the model output and to keep the dimensions consistent with the default 2000 draws.\nWe can also visually check model performance by first considering the trace plots. Given the number of parameters (500 x 12), we will only consider the hyperparameters.\n\n# Check population model trace plots.\ngamma_string <- str_c(\"Gamma[\", 1:data$J, \",\", 1, \"]\")\nomega_string <- str_c(\"Omega[\", 1:data$I, \",\", 1, \"]\")\ntau_string <- str_c(\"tau[\", 1:data$I, \"]\")\nfor (i in 2:data$I) {\n  gamma_temp <- str_c(\"Gamma[\", 1:data$J, \",\", i, \"]\")\n  gamma_string <- c(gamma_string, gamma_temp)\n  omega_temp <- str_c(\"Omega[\", 1:data$I, \",\", i, \"]\")\n  omega_string <- c(omega_string, omega_temp)\n}\n\n# Gamma.\nfit_noncentered %>%\n  mcmc_trace(\n    pars = gamma_string,\n    n_warmup = 500,\n    facet_args = list(\n      ncol = 4,\n      labeller = label_parsed\n    )\n  )\n\n\n\n# Omega.\nfit_noncentered %>%\n  mcmc_trace(\n    pars = omega_string,\n    n_warmup = 500,\n    facet_args = list(\n      ncol = 4,\n      labeller = label_parsed\n    )\n  )\n\n\nNote that the diagonal in a correlation matrix is fixed to 1.\n\n# tau.\nfit_noncentered %>%\n  mcmc_trace(\n    pars = tau_string,\n    n_warmup = 500,\n    facet_args = list(\n      ncol = 4,\n      labeller = label_parsed\n    )\n  )\n\n\nNow let’s demonstrate parameter recovery.\n\n# Recover Gamma values.\ngamma_values <- tibble(\n  j = sort(rep(1:(data$J), data$I)),\n  i = rep(1:(data$I), data$J),\n  .variable = str_c(\"Gamma\", \"_\", j, \"_\", i),\n  values = as.vector(t(matrix(sim_Gamma, ncol = data$I)))\n) %>%\n  select(.variable, values)\n\nfit_noncentered %>%\n  gather_draws(Gamma[j, i]) %>%\n  unite(.variable, .variable, j, i) %>%\n  ggplot(aes(x = .value, y = .variable)) +\n  geom_halfeyeh(.width = .95) +\n  geom_vline(aes(xintercept = values), gamma_values, color = \"red\") +\n  facet_wrap(\n    ~ .variable,\n    ncol = 4,\n    scales = \"free\"\n  )\n\n\n\n# Recover Omega values.\nomega_values <- tibble(\n  j = sort(rep(1:(data$I), data$I)),\n  i = rep(1:(data$I), data$I),\n  .variable = str_c(\"Omega\", \"_\", j, \"_\", i),\n  values = as.vector(t(matrix(sim_Omega, ncol = data$I)))\n) %>%\n  select(.variable, values)\n\nfit_noncentered %>%\n  gather_draws(Omega[j, i]) %>%\n  unite(.variable, .variable, j, i) %>%\n  ggplot(aes(x = .value, y = .variable)) +\n  geom_halfeyeh(.width = .95) +\n  geom_vline(aes(xintercept = values), omega_values, color = \"red\") +\n  facet_wrap(\n    ~ .variable,\n    ncol = 4,\n    scales = \"free\"\n  )\n\n\nOnce again, the diagonal in a correlation matrix is fixed at 1.\n\n# Recover tau values.\ntau_values <- tibble(\n  i = 1:(data$I),\n  .variable = str_c(\"tau\", \"_\", i),\n  values = as.vector(sim_tau)\n) %>%\n  select(.variable, values)\n\nfit_noncentered %>%\n  gather_draws(tau[i]) %>%\n  unite(.variable, .variable, i) %>%\n  ggplot(aes(x = .value, y = .variable)) +\n  geom_halfeyeh(.width = .95) +\n  geom_vline(aes(xintercept = values), tau_values, color = \"red\") +\n  facet_wrap(\n    ~ .variable,\n    ncol = 4,\n    scales = \"free\"\n  )\n\n\nWhile we have some difficulty recovering the variance parameters in our covariance matrix decomposition, which is typical, we can see that beyond HMC model diagnostics we have both good convergence and parameter recovery for our many hyperparameters."
  },
  {
    "objectID": "posts/choice-models/index.html#final-thoughts",
    "href": "posts/choice-models/index.html#final-thoughts",
    "title": "Comparing choice model parameterizations",
    "section": "Final thoughts",
    "text": "Final thoughts\nDoes it matter if we use a centered or non-centered parameterization for a hierarchical multinomial logit model? Yes. Based on HMC model diagnostics and using simulated data typical of a conjoint experiment, the centered parameterization is not sufficient. Additionally, though our primary concern isn’t computational efficiency, with both models running for 4000 iterations, the centered parameterization took 12 and a half hours to run while the non-centered parameterization took 5 hours to run.\nThis difference in choice model parameterizations is a concern because the centered parameterization is used almost exclusively in practice. However, since HMC isn’t as widespread, the diagnostics accompanying other estimation paradigms might not indicate any problems. This post doesn’t go so far as to establish the edge cases of when either parameterization might work best or consider how wrong the centered parameterization is. Regardless, given the savings in computation time and the evidence presented using simulated data, the non-centered parameterization should be our default approach in choice modeling.\n\n\nMarc Dotson\nMarc is an assistant professor of marketing at the BYU Marriott School of Business. He graduated with an MSc from The London School of Economics and Political Science in 2009 and a PhD from The Ohio State University in 2016. His research interests include Bayesian inference, predictive modeling, consumer preference heterogeneity, and unstructured data. Marc teaches marketing analytics. You can find him on Twitter and GitHub."
  },
  {
    "objectID": "posts/non-centered/index.html",
    "href": "posts/non-centered/index.html",
    "title": "Hierarchical models in Stan with a non-centered parameterization",
    "section": "",
    "text": "In a previous post, we provided a gentle introduction to hierarchical Bayesian models in Stan. We quickly ran into divergences (i.e., divergent transitions) when attempting to estimate our model. While hierarchical models inherently have posteriors with geometry that can be difficult to navigate, we were able to initially address this problem by more carefully navigating the posterior with a smaller step size.\nHowever, as a hierarchical model becomes more complicated, we need to re-express it in a way that is mathematically equivalent yet results in a posterior that is easier to navigate. We ended that previous post without relaxing the assumption of a common variance in the upper-level population model. In this post, we will build a hierarchical linear model with a multivariate population model. This added complexity will require us to re-express the model using what is known as a non-centered parameterization. Since this more flexible population model specification is standard, a non-centered parameterization should also be our default approach in most applications.\nOnce again, I am in debt to Michael Betancourt’s case studies, Richard McElreath’s Statistical Rethinking, the Stan User’s Guide, and Stan Discourse."
  },
  {
    "objectID": "posts/non-centered/index.html#multivariate-population-model",
    "href": "posts/non-centered/index.html#multivariate-population-model",
    "title": "Hierarchical models in Stan with a non-centered parameterization",
    "section": "Multivariate population model",
    "text": "Multivariate population model\nRecall that the motivation for hierarchical models is to allow for differences across groups while employing partial pooling, striking a balance between no pooling (separate flat models for each group) and complete pooling (a single flat model). However, so far we have assumed a common variance in the population model, limiting how much partial information pooling is possible. By moving to a multivariate population model, we allow for even more pooling through the covariance matrix.\nCovariance matrices can be difficult to work with, especially when it comes to setting a prior (or, in our case, a hyperprior). A helpful approach is to break down the covariance matrix into component pieces. Perhaps the most intuitive decomposition is to break down the covariance matrix into variances and a correlation matrix. If we have a covariance matrix Sigma, this decomposition works as follows:\nSigma = diag_matrix(tau) * Omega * diag_matrix(tau)\nwhere tau is a vector of scale parameters and Omega is a correlation matrix. Not only is this decomposition computationally more tractable, but it allows us to set a prior on tau and Omega separately rather than on Sigma directly.\nSo what does a hierarchical model with a multivariate population model look like in Stan?\n\n// Index values, observations, and covariates.\ndata {\n  int<lower = 1> N;               // Number of observations.\n  int<lower = 1> K;               // Number of groups.\n  int<lower = 1> I;               // Number of observation-level covariates.\n  int<lower = 1> J;               // Number of population-level covariates.\n\n  vector[N] y;                    // Vector of observations.\n  int<lower = 1, upper = K> g[N]; // Vector of group assignments.\n  matrix[N, I] X;                 // Matrix of observation-level covariates.\n  matrix[K, J] Z;                 // Matrix of population-level covariates.\n}\n\n// Parameters and hyperparameters.\nparameters {\n  matrix[J, I] Gamma;             // Matrix of population-level coefficients.\n  corr_matrix[I] Omega;           // Population model correlation matrix.\n  vector<lower = 0>[I] tau;       // Population model scale parameters.\n  matrix[K, I] Beta;              // Matrix of observation-level coefficients.\n  real<lower = 0> sigma;          // Variance of the likelihood.\n}\n\n// Hierarchical regression.\nmodel {\n  // Hyperpriors.\n  for (j in 1:J) {\n    Gamma[j,] ~ normal(0, 5);\n  }\n  Omega ~ lkj_corr(2);\n  tau ~ normal(0, 5);\n\n  // Prior.\n  sigma ~ normal(0, 5);\n\n  // Population model and likelihood.\n  for (k in 1:K) {\n    Beta[k,] ~ multi_normal(Z[k,] * Gamma, quad_form_diag(Omega, tau));\n  }\n  for (n in 1:N) {\n    y[n] ~ normal(X[n,] * Beta[g[n],]', sigma);\n  }\n}\n\nWe’ll save this Stan script as hlm_centered.stan (where “hlm” refers to a hierarchical linear model and we’ll explain the “centered” part shortly). In the parameters block, we have a correlation matrix Omega and tau is now a vector of scale parameters rather than a scalar. In the models block, Omega is distributed according to the LKJ distribution, a special multivariate Beta-like distribution for correlation matrices. Additionally, Beta is now distributed multivariate normal, with the covariance matrix a recombination of Omega and tau as described above.\nSince all Bayesian models are generative, we can translate this Stan script into data and generated quantities blocks and use Stan to generate data for us.\n\n// Index values, covariates, and hyperparameter values.\ndata {\n  int<lower = 1> N;               // Number of observations.\n  int<lower = 1> K;               // Number of groups.\n  int<lower = 1> I;               // Number of observation-level covariates.\n  int<lower = 1> J;               // Number of population-level covariates.\n\n  int<lower = 1, upper = K> g[N]; // Vector of group assignments.\n  matrix[N, I] X;                 // Matrix of observation-level covariates.\n  matrix[K, J] Z;                 // Matrix of population-level covariates.\n}\n\n// Generate data according to the hierarchical regression.\ngenerated quantities {\n  vector[N] y;                    // Vector of observations.\n  matrix[J, I] Gamma;             // Matrix of population-level coefficients.\n  corr_matrix[I] Omega;           // Population model correlation matrix.\n  vector[I] tau;                  // Population model scale parameters.\n  matrix[K, I] Beta;              // Matrix of group-level coefficients.\n  real<lower = 0> sigma;          // Variance of the likelihood.\n\n  // Draw parameter values and generate data.\n  for (j in 1:J) {\n    for (i in 1:I) {\n      Gamma[j, i] = normal_rng(0, 5);\n    }\n  }\n  Omega = lkj_corr_rng(I, 2);\n  for (i in 1:I) {\n    tau[i] = chi_square_rng(2);\n  }\n  for (k in 1:K) {\n    Beta[k,] = multi_normal_rng(Z[k,] * Gamma, quad_form_diag(Omega, tau))';\n  }\n  sigma = normal_rng(0, 5);\n  for (n in 1:N) {\n    y[n] = normal_rng(X[n,] * Beta[g[n],]', sigma);\n  }\n}\n\nWe’ll save this Stan script as generate_data.stan. Note that instead of specifying parameter values in the data block, we are generating them in generated quantities. This includes using the LKJ prior to generate the values of the correlation matrix Omega. Also, note that since Beta is a matrix where each row is an observation, the vector output of multi_normal_rng is transposed.\nIn an R script, let’s load the necessary packages, allow Stan to use as many cores as we have available, allow for Stan to save compiled code, specify assumed parameter values, generate data according to our hierarchical linear model by calling generate_data.stan, and estimate our model by calling hlm_centered.stan.\n\n# Load packages.\nlibrary(tidyverse)\nlibrary(rstan)\nlibrary(bayesplot)\nlibrary(tidybayes)\n\n# Set Stan options.\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\n# Specify data and hyperparameter values.\nsim_values <- list(\n  N = 500,                            # Number of observations.\n  K = 5,                              # Number of groups.\n  I = 7,                              # Number of observation-level covariates.\n  J = 3,                              # Number of population-level covariates.\n\n  # Vector of group assignments.\n  g = sample(5, 500, replace = TRUE),\n  \n  # Matrix of observation-level covariates.\n  X = cbind(\n    rep(1, 500),\n    matrix(runif(500 * (7 - 1), min = 1, max = 10), nrow = 500)\n  ),\n\n  # Matrix of population-level covariates.\n  Z = cbind(\n    rep(1, 5),\n    matrix(runif(5 * (3 - 1), min = 2, max = 5), nrow = 5)\n  )\n)\n\n# Generate data.\nsim_data <- stan(\n  file = here::here(\"Code\", \"generate_data.stan\"),\n  data = sim_values,\n  iter = 1,\n  chains = 1,\n  seed = 42,\n  algorithm = \"Fixed_param\"\n)\n\n# Extract simulated data and parameters.\nsim_y <- extract(sim_data)$y\nsim_Gamma <- extract(sim_data)$Gamma\nsim_Omega <- extract(sim_data)$Omega\nsim_tau <- extract(sim_data)$tau\nsim_Beta <- extract(sim_data)$Beta\nsim_sigma <- extract(sim_data)$sigma\n\ndata <- list(\n  N = sim_values$N,     # Number of observations.\n  K = sim_values$K,     # Number of groups.\n  I = sim_values$I,     # Number of observation-level covariates.\n  J = sim_values$J,     # Number of population-level covariates.\n  y = as.vector(sim_y), # Vector of observations.\n  g = sim_values$g,     # Vector of group assignments.\n  X = sim_values$X,     # Matrix of observation-level covariates.\n  Z = sim_values$Z      # Matrix of population-level covariates.\n)\n\nfit_centered <- stan(\n  file = here::here(\"Code\", \"hlm_centered.stan\"),\n  data = data,\n  control = list(adapt_delta = 0.99),\n  seed = 42\n)\n\nWarning messages:\n1: There were 119 divergent transitions after warmup. Increasing adapt_delta above 0.99 may help. See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup \n2: There were 3577 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded \n3: Examine the pairs() plot to diagnose sampling problems\n \n4: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. Running the chains for more iterations may help. See http://mc-stan.org/misc/warnings.html#bulk-ess \nRunning this model gives us more than 100 divergent transitions. Recall that a divergent transition or divergence is a unique Hamiltonian Monte Carlo diagnostic that identifies problems navigating the posterior distribution. These difficulties with posterior geometry are true regardless of the sampler, but Hamiltonian Monte Carlo makes the issue transparent.\nWe have already set adapt_delta = 0.99, which is the smaller step size we set previously. In order to produce a posterior geometry that can be navigated, we need to reparameterize our model."
  },
  {
    "objectID": "posts/non-centered/index.html#non-centered-parameterization",
    "href": "posts/non-centered/index.html#non-centered-parameterization",
    "title": "Hierarchical models in Stan with a non-centered parameterization",
    "section": "Non-centered parameterization",
    "text": "Non-centered parameterization\nThe centered parameterization for a hierarchical linear model, as expressed above, has a population model and likelihood:\nBeta ~ multi_normal(mu, Sigma)\ny ~ normal(Beta, sigma)\nwhere we get draws from the posterior distribution of mu and Sigma, the population mean (where we specified mu = Z * Gamma) and covariance (where we specified Sigma = diag_matrix(tau) * Omega * diag_matrix(tau)) and Beta, the group-level coefficients.\nThe non-centered parameterization re-expresses the population model and likelihood for a hierarchical linear model as:\nDelta ~ normal(0, 1)\nBeta = mu + Delta * Sigma\ny ~ normal(Beta, sigma)\nwhere we get draws from the posterior distribution of mu, Sigma, and Delta, since Beta is now a deterministic transformation of the other parameters (i.e., we have Beta = instead of Beta ~). The benefit of a non-centered parameterization – the inclusion of the intermediate Delta and the deterministic transformation of Beta – is that difficult dependencies between the two layers in the hierarchy are broken, producing a simpler posterior geometry.\nSo what does our hierarchical linear model look like with a non-centered parameterization?\n\n// Index values, observations, and covariates.\ndata {\n  int<lower = 1> N;               // Number of observations.\n  int<lower = 1> K;               // Number of groups.\n  int<lower = 1> I;               // Number of observation-level covariates.\n  int<lower = 1> J;               // Number of population-level covariates.\n\n  vector[N] y;                    // Vector of observations.\n  int<lower = 1, upper = K> g[N]; // Vector of group assignments.\n  matrix[N, I] X;                 // Matrix of observation-level covariates.\n  matrix[K, J] Z;                 // Matrix of population-level covariates.\n}\n\n// Parameters and hyperparameters.\nparameters {\n  matrix[J, I] Gamma;             // Matrix of population-level coefficients.\n  corr_matrix[I] Omega;           // Population model correlation matrix.\n  vector<lower = 0>[I] tau;       // Population model scale parameters.\n  matrix[K, I] Delta;             // Matrix of observation-level coefficients.\n  real<lower = 0> sigma;          // Variance of the likelihood.\n}\n\n// Deterministic transformation.\ntransformed parameters {\n  // Matrix of observation-level coefficients.\n  matrix[K, I] Beta;\n\n  // Non-centered parameterization.\n  for (k in 1:K) {\n    Beta[k,] = Z[k,] * Gamma + Delta[k,] * quad_form_diag(Omega, tau);\n  }\n}\n\n// Hierarchical regression.\nmodel {\n  // Hyperpriors.\n  for (j in 1:J) {\n    Gamma[j,] ~ normal(0, 5);\n  }\n  Omega ~ lkj_corr(2);\n  tau ~ normal(0, 5);\n\n  // Prior.\n  sigma ~ normal(0, 5);\n\n  // Non-centered population model and likelihood.\n  for (k in 1:K) {\n    Delta[k,] ~ normal(0, 1);\n  }\n  for (n in 1:N) {\n    y[n] ~ normal(X[n,] * Beta[g[n],]', sigma);\n  }\n}\n\nThe parameters block now contains Delta instead of Beta, the matrix of non-centered observation-level coefficients. We have an entirely new block transformed parameters that includes our specification of Beta and the deterministic transformation. Finally, the population model in the model block is replaced by a standard normal draw.\nWe don’t need to generated new data – the generative model remains the same, it’s the model that has been re-expressed with the new parameterization. Let’s call this new script hlm_noncentered.stan from R.\n\nfit_noncentered <- stan(\n  file = here::here(\"Code\", \"hlm_noncentered.stan\"),\n  data = data,\n  control = list(adapt_delta = 0.99),\n  seed = 42\n)\n\nWarning messages:\n1: There were 4000 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded \n2: Examine the pairs() plot to diagnose sampling problems\n \n3: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. Running the chains for more iterations may help. See http://mc-stan.org/misc/warnings.html#bulk-ess\nHuzzah! Divergences are gone. We do have some warnings about max_treedepth, which is a warning about efficiency rather than model validity (more on that at the end), and a notice about Bulk Effective Sample Size that means we should probably run the model longer to establish convergence. To address this latter problem, we can use the iter argument (the default is 2000). Also, if we want to manage the model output file size, we can use thin to keep every few draws instead of saving all of them.\nLet’s re-run the model.\n\nfit_noncentered <- stan(\n  file = here::here(\"Code\", \"hlm_noncentered.stan\"),\n  data = data,\n  iter = 10000,\n  thin = 5,\n  control = list(adapt_delta = 0.99),\n  seed = 42\n)\n\nWarning messages:\n1: There were 4000 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded \n2: Examine the pairs() plot to diagnose sampling problems\nThe model takes longer to run, but we are only left with a warning about efficiency.\nAs before, let’s evaluate trace plots.\n\n# Check population model trace plots.\ngamma_string <- str_c(\"Gamma[\", 1:data$J, \",\", 1, \"]\")\nomega_string <- str_c(\"Omega[\", 1:data$I, \",\", 1, \"]\")\ntau_string <- str_c(\"tau[\", 1:data$I, \"]\")\nfor (i in 2:data$I) {\n  gamma_temp <- str_c(\"Gamma[\", 1:data$J, \",\", i, \"]\")\n  gamma_string <- c(gamma_string, gamma_temp)\n  omega_temp <- str_c(\"Omega[\", 1:data$I, \",\", i, \"]\")\n  omega_string <- c(omega_string, omega_temp)\n}\n\n# Gamma.\nfit_noncentered %>%\n  mcmc_trace(\n    pars = gamma_string,\n    n_warmup = 500,\n    facet_args = list(\n      nrow = ceiling(length(gamma_string) / 4),\n      ncol = 4,\n      labeller = label_parsed\n    )\n  )\n\n\n\n# Omega.\nfit_noncentered %>%\n  mcmc_trace(\n    pars = omega_string,\n    n_warmup = 500,\n    facet_args = list(\n      nrow = ceiling(length(omega_string) / 4),\n      ncol = 4,\n      labeller = label_parsed\n    )\n  )\n\n\nNote that the diagonal in a correlation matrix is fixed to 1.\n\n# tau.\nfit_noncentered %>%\n  mcmc_trace(\n    pars = tau_string,\n    n_warmup = 500,\n    facet_args = list(\n      nrow = ceiling(length(tau_string) / 4),\n      ncol = 4,\n      labeller = label_parsed\n    )\n  )\n\n\n\n# Check observation model trace plots.\nbeta_string <- str_c(\"Beta[\", 1:data$K, \",\", 1, \"]\")\nfor (i in 2:data$I) {\n  beta_temp <- str_c(\"Beta[\", 1:data$K, \",\", i, \"]\")\n  beta_string <- c(beta_string, beta_temp)\n}\n\n# Beta and sigma.\nfit_noncentered %>%\n  mcmc_trace(\n    pars = c(beta_string, \"sigma\"),\n    n_warmup = 500,\n    facet_args = list(\n      nrow = ceiling(length(c(beta_string, \"sigma\")) / 4),\n      ncol = 4,\n      labeller = label_parsed\n    )\n  )\n\n\nAcross population and observation model parameters, we have good mixing and clear convergence across all chains. Now let’s demonstrate parameter recovery for our many model parameters.\n\n# Recover Gamma values.\ngamma_values <- tibble(\n  j = sort(rep(1:(data$J), data$I)),\n  i = rep(1:(data$I), data$J),\n  .variable = str_c(\"Gamma\", \"_\", j, \"_\", i),\n  values = as.vector(t(matrix(sim_Gamma, ncol = data$I)))\n) %>%\n  select(.variable, values)\n\nfit_noncentered %>%\n  gather_draws(Gamma[j, i]) %>%\n  unite(.variable, .variable, j, i) %>%\n  ggplot(aes(x = .value, y = .variable)) +\n  geom_halfeyeh(.width = .95) +\n  geom_vline(aes(xintercept = values), gamma_values, color = \"red\") +\n  facet_wrap(\n    ~ .variable,\n    nrow = data$J,\n    ncol = data$I,\n    scales = \"free\"\n  )\n\n\n\n# Recover Omega values.\nomega_values <- tibble(\n  j = sort(rep(1:(data$I), data$I)),\n  i = rep(1:(data$I), data$I),\n  .variable = str_c(\"Omega\", \"_\", j, \"_\", i),\n  values = as.vector(t(matrix(sim_Omega, ncol = data$I)))\n) %>%\n  select(.variable, values)\n\nfit_noncentered %>%\n  gather_draws(Omega[j, i]) %>%\n  unite(.variable, .variable, j, i) %>%\n  ggplot(aes(x = .value, y = .variable)) +\n  geom_halfeyeh(.width = .95) +\n  geom_vline(aes(xintercept = values), omega_values, color = \"red\") +\n  facet_wrap(\n    ~ .variable,\n    nrow = data$I,\n    ncol = data$I,\n    scales = \"free\"\n  )\n\n\nOnce again, the diagonal in a correlation matrix is fixed at 1.\n\n# Recover tau values.\ntau_values <- tibble(\n  i = 1:(data$I),\n  .variable = str_c(\"tau\", \"_\", i),\n  values = as.vector(sim_tau)\n) %>%\n  select(.variable, values)\n\nfit_noncentered %>%\n  gather_draws(tau[i]) %>%\n  unite(.variable, .variable, i) %>%\n  ggplot(aes(x = .value, y = .variable)) +\n  geom_halfeyeh(.width = .95) +\n  geom_vline(aes(xintercept = values), tau_values, color = \"red\") +\n  facet_wrap(\n    ~ .variable,\n    nrow = ceiling(data$I / 4),\n    ncol = 4,\n    scales = \"free\"\n  )\n\n\nWe have some difficulty recovering the variance parameters in our covariance matrix decomposition.\n\n# Recover Beta values.\nbeta_values <- tibble(\n  n = sort(rep(1:(data$K), data$I)),\n  i = rep(1:(data$I), data$K),\n  .variable = str_c(\"Beta\", \"_\", n, \"_\", i),\n  values = as.vector(t(matrix(sim_Beta, ncol = data$I)))\n) %>%\n  select(.variable, values)\n\nfit_noncentered %>%\n  gather_draws(Beta[n, i]) %>%\n  unite(.variable, .variable, n, i) %>%\n  ggplot(aes(x = .value, y = .variable)) +\n  geom_halfeyeh(.width = .95) +\n  geom_vline(aes(xintercept = values), beta_values, color = \"red\") +\n  facet_wrap(\n    ~ .variable,\n    nrow = data$K,\n    ncol = data$I,\n    scales = \"free\"\n  )\n\n\n\n# Recover sigma value.\nsigma_values <- tibble(\n  .variable = \"sigma\",\n  values = sim_sigma,\n)\n\nfit_noncentered %>%\n  gather_draws(sigma) %>%\n  ggplot(aes(x = .value, y = .variable)) +\n  geom_halfeyeh(.width = .95) +\n  geom_vline(aes(xintercept = values), sigma_values, color = \"red\") +\n  facet_wrap(\n    ~ .variable,\n    nrow = 1,\n    scales = \"free\"\n  )\n\n\nBesides the tau parameters (and variances can be notoriously difficult) we have good convergence and model recovery. To summarize, we have now specified a hierarchical linear model with a multivariate upper-level, generated data according to that model, and used the generated data to demonstrate that the model is working by employing a non-centered parameterization."
  },
  {
    "objectID": "posts/non-centered/index.html#final-thoughts",
    "href": "posts/non-centered/index.html#final-thoughts",
    "title": "Hierarchical models in Stan with a non-centered parameterization",
    "section": "Final thoughts",
    "text": "Final thoughts\nThe combination of the previous post and this one hopefully provide a helpful and more complete starting point for using hierarchical models in Stan. The max_treedepth warning highlights that all of this code isn’t the most efficient. However, it is often best to worry about readability before optimizing code for efficiency.\nThe need to impose the non-centered parameterization to break difficult dependencies between the two layers in the hierarchy grows with thinner data or more dimensions, as we saw by introducing a multivariate population model with a covariance matrix. However, the centered and non-centered parameterizations are inversely related in terms of efficiency; when a centered parameterization will suffice, a non-centered parameterization should underperform and when a non-centered parameterization will suffice, a centered parameterization should underperform.\nGiven that a hierarchical model with a multivariate population model should be our default, so should the requisite non-centered parameterization. This need is true irrespective of the sampler. Using Stan and Hamiltonian Monte Carlo just makes this apparent with the divergences diagnostic.\n\n\nMarc Dotson\nMarc is an assistant professor of marketing at the BYU Marriott School of Business. He graduated with an MSc from The London School of Economics and Political Science in 2009 and a PhD from The Ohio State University in 2016. His research interests include Bayesian inference, predictive modeling, consumer preference heterogeneity, and unstructured data. Marc teaches marketing analytics. You can find him on Twitter and GitHub."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Occasional Divergences",
    "section": "",
    "text": "r\n\n\nstan\n\n\nhierarchical models\n\n\nchoice models\n\n\n\n\n\n\n\n\n\n\n\n02 Sep 2020\n\n\n25 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nr\n\n\nstan\n\n\nhierarchical models\n\n\n\n\n\n\n\n\n\n\n\n19 May 2020\n\n\n19 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nr\n\n\nstan\n\n\nhierarchical models\n\n\n\n\n\n\n\n\n\n\n\n31 Dec 2019\n\n\n26 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Two brothers with a mostly Bayesian focus.",
    "section": "",
    "text": "Jeff started it. He didn’t know what Bayesian inference was, but he was interested in learning. Naturally he shared his excitement with his little brother, Marc, who (as usual) followed in his footsteps. Now they share that interest working together at the intersection of business and data science and, occasionally, writing on this blog."
  },
  {
    "objectID": "about.html#jeff-dotson",
    "href": "about.html#jeff-dotson",
    "title": "Two brothers with a mostly Bayesian focus.",
    "section": "Jeff Dotson",
    "text": "Jeff Dotson\nJeff is a professor of marketing at the BYU Marriott School of Business where he teaches courses in pricing strategies, marketing research, and marketing analytics. Prior to joining the faculty at BYU, he worked as an assistant professor in the Owen Graduate School of Management at Vanderbilt University. He holds a PhD in Marketing from The Ohio State University and masters degrees in business administration (MBA) and statistics (MS) from the University of Utah. His research focuses on the development and application of Bayesian statistical methods to a variety of marketing problems."
  },
  {
    "objectID": "about.html#marc-dotson",
    "href": "about.html#marc-dotson",
    "title": "Two brothers with a mostly Bayesian focus.",
    "section": "Marc Dotson",
    "text": "Marc Dotson\nMarc is an assistant professor of marketing at the BYU Marriott School of Business. He graduated with an MSc from The London School of Economics and Political Science in 2009 and a PhD from The Ohio State University in 2016. His research interests include Bayesian inference, predictive modeling, consumer preference heterogeneity, and unstructured data. Marc teaches marketing analytics."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "Content is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License and may be reused as long as authorship is indicated and a link is provided back to the original material. Code is licensed under the MIT License and may be reused “as is” without restriction."
  }
]