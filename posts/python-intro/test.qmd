---
title: "Untitled"
format: html
---

```{r}
library(tidyverse)
library(tidymodels)

# Set seed, variable, and parameter values.
set.seed(42)
nobs <- 500
beta0 <- -5
beta1 <- 5
beta2 <- 2
beta3 <- 0

# Simulate data.
sim_data <- tibble(
  x1 = round(runif(nobs, min = 0, max = 20)),
  x2 = rbinom(nobs, size = 2, prob = c(0.7, 0.3)) |> 
    as.factor() |> fct_recode("level01" = "0", "level02" = "1", "level03" = "2"),
  y = beta0 + beta1 * x1 + beta2 * ifelse(x2 == "level02", 1, 0) + beta3 * ifelse(x2 == "level03", 1, 0) + rnorm(nobs, mean = 0, sd = 3)
)

# Training and testing split.
sim_split <- initial_split(sim_data, prop = 0.90)

# Feature engineering.
sim_recipe <- training(sim_split) |>
  recipe(y ~ .) |> 
  step_dummy(all_nominal_predictors())

# Model specification.
sim_lm <- linear_reg() |> 
  set_engine("lm")

# Compose a workflow.
sim_wf_lm <- workflow() |> 
  add_recipe(sim_recipe) |> 
  add_model(sim_lm)

# Fit the model.
sim_lm_fit <- fit(sim_wf_lm, data = training(sim_split))

# Visualize slope parameter estimates.
tidy(sim_lm_fit, conf.int = TRUE) |> 
  ggplot(aes(x = term)) + 
  geom_point(aes(y = estimate)) + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .1) +
  geom_hline(yintercept = 0, color = "red")

# Compute RMSE.
sim_lm_fit |> 
  predict(new_data = testing(sim_split)) |>
  bind_cols(testing(sim_split)) |>
  mse(truth = y, estimate = .pred)

# Column names need to match the original data.
new_data <- tibble(
  x1 = rep(c(10, 20, 30, 40), 3),
  x2 = c(rep("level01", 4), rep("level02", 4), rep("level03", 4))
)

# Predict and bind on prediction intervals.
predict(sim_lm_fit, new_data = new_data) |> 
  bind_cols(
    predict(sim_lm_fit, new_data = new_data, type = "pred_int"),
    new_data
  ) |> 
  arrange(desc(.pred))
```

```{python}
import polars as pl
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error

# Set seed, variable, and parameter values.
np.random.seed(42)
nobs = 500
beta0 = -5
beta1 = 5
beta2 = 2
beta3 = 0

# Simulate data.
sim_data = pl.DataFrame({
    'x1': np.round(np.random.uniform(0, 20, nobs)),
    'x2': np.random.choice(['level01', 'level02', 'level03'], nobs, p=[0.7, 0.2, 0.1]),
    'y': beta0 + beta1 * np.random.uniform(0, 20, nobs) + beta2 * (np.random.choice(['level01', 'level02', 'level03'], nobs, p=[0.7, 0.2, 0.1]) == 'level02') + beta3 * (np.random.choice(['level01', 'level02', 'level03'], nobs, p=[0.7, 0.2, 0.1]) == 'level03') + np.random.normal(0, 3, nobs)
})

# Training and testing split.
X = sim_data.drop('y').to_pandas()
y = sim_data['y'].to_pandas()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Feature engineering.
numeric_features = ['x1']
categorical_features = ['x2']

numeric_transformer = Pipeline(steps=[
    ('imputer', None)  # No imputation needed for numeric features
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

# Model specification.
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', LinearRegression())
])

# Fit the model.
model.fit(X_train, y_train)

# Visualize slope parameter estimates.
print(pl.DataFrame(model.named_steps['regressor'].coef_, ['intercept', 'x1', 'x2_level02', 'x2_level03']))

# Compute MSE.
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(mse)

# Column names need to match the original data.
new_data = pl.DataFrame({
    'x1': np.repeat([10, 20, 30, 40], 3),
    'x2': np.tile(['level01', 'level02', 'level03'], 4)
})

# Predict and bind on prediction intervals.
y_pred = model.predict(new_data)
y_pred_int = model.predict(new_data)
y_pred_int[:, 0] = y_pred - 1.96 * np.std(y_pred)
y_pred_int[:, 1] = y_pred + 1.96 * np.std(y_pred)
print(pl.DataFrame({'y_pred': y_pred, 'y_pred_int': y_pred_int}))
```
