---
title: "Spinning up and using virtual machines for R and Python"
author: "Marc Dotson"
date: "2024-10-31"
description: |
  Breaking through computational bottlenecks can be challenging, but spinning up and using a virtual machine comes with its own set of problems. This post walks through the process of spinning up and using virtual machines for R and Python.
categories:
  - r
  - python
  - compute
image: figures/owl.png
slug: virtual-machines
---

It's not uncommon to run into computational bottlenecks when running models, Bayesian or otherwise. I've often had access to a dedicated computer or server for such tasks or, in classic academic fashion, I've been comfortable with letting models run for weeks. But these options aren't always available and they might not actually solve the problem. Cloud computing is clearly the solution. However, breaking into cloud computing has made me feel the need to (with apologies) adapt the adage often applied to using regular expressions: "If you have a problem that requires cloud computing, congratulations, you now have two problems."

There are a number of cloud service providers, AWS arguably chief among them. However, AWS can do *everything* when what I really need is the ability to quickly spin up and temporarily use a virtual machine that can easily run R, Python, and often Stan. One of the primary reason VS Code has been so hyped is the use of its SSH extension for easily tunneling into and using such a virtual machine. However, the virtual machine needs to be spun up in the first place. In short, using cloud computing definitely feels like a *draw the rest of the owl* situation.

![](figures/owl.png){width=50% fig-align="center"}

In this post, I try to fill in the gaps and provide a gentle introduction to spinning up and using virtual machines that can run R and Python. Special thanks to Sharad Jones, Lizzy Nguyen, and Andrew Heiss for helping me navigate all of this.

## Spinning up virtual machines

A virtual machine (VM) is a computer that isn't *actually* a computer---it's rented time on a server in a data center somewhere, hence it being "virtual." But before we can set (or spin) up a VM, we need to prepare the machine image we want to work with on the VM. This image is really a [reproducible environment](https://occasionaldivergences.com/posts/rep-env/) writ large: It needs to include not only the required programming languages and packages, but also the operating system and necesssary applications.

::: {.callout-note title="Image Requirements?"}
Do we actually need to specify the operating system and applications for an image? If I SSH into a machine from an IDE, shouldn't that IDE be all that I need, regardless of whatever is installed at the other end? To that end, do I really even need Docker at that point?

I'm loooking for the simplest possible way to get computation started, and certainly having to create an image is just another bottleneck.

Sharad's AWS/EC2 walkthrough:

- EC2 instance with a selected AMI (Amazon Machine Image)
- Ubuntu OS (not a WSL Linux image for Positron) plus image specifics
- Note free tier options, which architecture to select (x86 or Arm)?
- Instance type appropriate for my work?
- Create a new key pair for SSH connection using RSA and .pem for OpenSSH.
- Create or use an existing security group and allow SSH traffic. Maybe from My IP.
- Configure storage is for the EBS volume. Doesn't need to be tied to the EC2 instance.
- Port 22 is specific for inbound traffic using SSH.
- chmod 400 “keypair.pem” is UNIX-specific (read-only access) for SSH connection?

Questions:

1. Do I still initialize the use of my key pair using chmod 400 “keypair.pem” via the terminal, or is that baked in elswhere in the remote SSH workflow?
2. What is it that I input when tunneling in VS Code? It's asking for something akin to `user@hostname:port`.

Additional notes:

- EBS volume on Amazon for external connection
- AMI as a launch script instead of a Docker image
- S3 to transfer quickly to EBS volumes on EC2

- 
:::

You can think of an image as a *snapshot* of everything you need to run your model. That means once you have an image, it can be used to run models isolated from everything else. These self-contained images are known as *containers*. Designed for development and production well beyond the scope of what we're discussing, the fact that they have everything they need to run in isolation means you can have multiple containers running on your computer, actual or virtual.

There are a number of applications for creating images and "containerizing" them, Docker arguably chief among them. But, to adapt Jenny Bryan's adage about for loops: "Someone needs to build an image, but it doesn't have to be you." There are a plethora of pre-built images available on Docker Hub. It's like GitHub, but specifically for images.

### 1. Create/modify an image using Docker Compose?

This isn't a tutorial for creating a Docker image. In the short term, I need a VS Code image that's ready to run Python (bonus for R and Stan). In the long term, once SSH tunneling is a thing in Positron, I'll want to modify my own template, include it as a repo on GitHub, and have it connected to Docker Hub.

- Here's one for just R and VS Code: https://github.com/RamiKrispin/vscode-r

> He's got Dockerfiles in the different example folders (github.com/RamiKrispin/...) and you build them yourself (right click on the Dockerfile in VS Code and choose Build Image…)
> 
> I set up docker on the remote computer and then build the Docker container on the remote computer using an SSH-connected VS Code window

- Here's an example from Andrew using R, Python, and Quarto: https://github.com/andrewheiss/example-r-python-quarto-docker
- Andrew's old blog post: https://www.andrewheiss.com/blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/index.html

### 2. Use the image as a container when spinning up the VM?

> As far as we know, no one's made a Docker + R + Python + Positron. But you don't really need Positron in the image, just R and Python -- Positron downloads its server component into the remote host as part of establishing a session. -Jonathan @ Posit

- Spin up the container using Docker. Official Digital Ocean documentation to link?
- Log in using the Droplet's public IPv4 address: `$ ssh root@__.___.__.___`
- Pull the Docker image you want to use `$ docker pull rocker/tidyverse`
- Run the image at a specific port `$ docker run -d -p 8787:8787 rocker/tidyverse`
- Access the remote installation at `__.___.__.___:8787`

::: {.callout-note title="AWS is Too General?"}
I feel like this is similar to the complaints I make about VS Code. Add some guidelines on how to do this in AWS to the callout note?

AWS, like Azure and GCP (is it too general)? Can use AWS for *anything*. App hosting, etc. Including using GPU clusters and quantum computing! Cutting a cake with a chainsaw.

- Andrew Brown's AWS Certified Cloud Practitioner Certification Course: https://www.youtube.com/watch?v=SOTamWNgDKc&list=PLWKjhJtqVAbkzvvpY12KkfiIGso9A_Ixs&index=2
- Elastic Compute Cloud (EC2) for VM or Elastic Constainer Service (ECS) for an EC2 instance with Docker installed.
- What about higher performance computing services (HPC)? Does it only come in clusters? Would I need to use CloudShell?
- S3/RDS for storage.
- Have to pick an OS, instance type, add storage...Docker?
- Memory optimized VMs have large data sets in memory.

Digital Ocean? From Andrew:

- Look for a free tier or (initial credit)[https://cloud.digitalocean.com/registrations/new?refcode=f6fcd01aaffb].
- Like AWS, you'll have a team (root user) and members in that team, which will be your way to access and use the service.
:::

## Using virtual machines

### 1. SSH tunnel in to use the container?

- SSH Tunneling with VS Code: https://www.youtube.com/watch?v=cOopQQIL8JU

SSH tunneling is available experimentally in Positron: https://github.com/posit-dev/positron/pull/4251 using a "lightly modified" version of Open Remote SSH (https://github.com/jeanp413/open-remote-ssh) that's already bundled into the install.

> When credentials are supplied to a remote host, Positron will connect to the host, download a server binary from Positron's Github Releases page matching the version of Positron being used on the client side, unpack the server into ~/.positron-server, then start it inside the remote host.
> 
> Because this change is experimental, the UI for connecting to remote hosts (in the lower left corner) continues to be disabled by default. To show the UI, turn on this option: "Positron: Remote Host Experimental" to "Enable support for connecting to remote hosts over SSH."
>
> Only Linux hosts on 64-bit Intel (x86_64) CPU architectures are supported right now.

May also need to install a supported SSH client: https://code.visualstudio.com/docs/remote/troubleshooting#_installing-a-supported-ssh-client. It comes built-in for macOS.

- Bottom left corner shows connection.
- Open SSH Configuration File:

Host my-server
    HostName your.server.ip.address
    User root

- 

### 2. How do I get data into and out of the container?

- What about getting an S3 instance? Is that what the EBS volume is for?
- Andrew likes to mount local folders to the docker container so he knows where everything rather than use official dev envirnoment tools that have more opaque file systems. That said, most of his work is for testing things for package development, not for computational resources.
- Note on scaling up the size of the VM.

## Final thoughts

- Consider using extensions for managing VMs inside Positron?

