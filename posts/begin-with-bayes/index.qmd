---
title: "Begin with Bayes"
author: "Marc Dotson"
date: "2025-05-30"
description: |
  Begin with Bayes.
categories:
  - bayesian inference
  - pml
image: figures/positron-logo.png
slug: begin-with-bayes
---

Introduction to the idea of facilitating statistical literacy by beginning with a unifying probabilistic lense that leads into Bayesian inference. Collect resources and links. A teaching philosophy. The beginnings of a pedagogical paper. This is a deliberate homage to David Robinson's [Teach the tidyverse to beginners](http://varianceexplained.org/r/teach-tidyverse/) blog post. Even if it's more computational burdensome, Bayesian inference is more intuitive. Even if you do teach frequentist inference, most students don’t get it anyway. Especially for applied students, we need to teach everything from scratch at a “self-contained conceptual level.” Motivate with connecting probability for quantifying uncertainty to decision making; a need to understand epistemology -- how we even know things.

- See https://www.the100.ci/2024/08/27/lets-do-statistics-the-other-way-around/
- See https://elevanth.org/blog/2021/06/15/regression-fire-and-dangerous-things-1-3/
- See https://www.tandfonline.com/doi/full/10.1080/29932955.2024.2370620
- A need to build intuition for non-technical students.
- The necessity of intuition having a "unifying principle" to build on.
- Who said that Bayesian inference is what happens when you take probability seriously?
- How to introduce Bayes without using frequentist language (McElreath).
- Not treating Bayesian inference as an advanced topic that comes after frequentist inference.
- Can I compare frequentism to Trinitarian ideas? Not intuitive but hard to let go of once they take root. Does that make Bayesianism the Restoration? Bringing back the original.

## DATA 5600 Revamp Outline

- Overview of course: ...
- Probability and epistemology
- Statistical modeling
- Decision theory

## Bayes is dead. Long live Bayes.

> "Bayesian inference is what happens when you take probability seriously." -Steve or Richard? Unknown.

*Regression and Other Stories* is frustrating. It employs Bayesian concepts early, but presents them using the language of frequentist inference. This is likely applicable to a certain audience already well versed in frequentist language, like using Catholic terminology to explain the Restoration. However, it conflates the probabilistic foundations that give rise to Bayesian inference as a natural extension.

*Probabilistic Machine Learning* provides a genuinely unifying principle by incorporating all of statistics and machine learning within the language and probability. Only from a probabilistic perspective can we see Bayesian inference as a natural extension, without the baggage of making the Bayesian approach fit into frequentist terminology, as if frequentist terminology were the truth. This is for a difference audience, once that isn't committed to or have an understanding of the frequentist approach.

I'm assuming *An Introduction to Statistical Learning* takes another tack, and tries to unify statistical and machine learning concepts from a decidedly ML perspective.

## Probability

> “Counting the number of ways things can happen, according to our assumptions.” (p. 11, Statistical Rethinking)

Probability is a formal way to quantify uncertainty. A probability for a possible outcome needs to be nonnegative (i.e., zero or a positive number) and the sum of the probabilities for all possible outcomes needs to sum to 1. A probability distribution is a list of all possible outcomes together with the associated probabilities. Not surprisingly, this might be easiest to see with a visualization. Note how the width or variance in a probability distribution is an expression of uncertainty. The more variance in the probability distribution, the more uncertain we are. The greatest expression of uncertainty is that every outcome is equally possible (i.e., a uniform distribution).

The use of probability as a unifying lense has a great argument epistemologically. Not that probabilities actually exist -- wave functions excluded? the mind of God? -- but they are a self-consistent, principled way to approach quantifying uncertainty and learning about unknowns.

- Instead of a likelihood function, an information transfer function.
- Why is the likelihood function represented then with distribution notation? Because it is a probability distribution before we evaluate it with actual data, then it's "likelihood" because it gives us how likely the data are given different values of the parameters, but doesn't sum to one. Hence the need to normalize the prior distribution times the likelihood to produce the posterior distribution.

## Statistics

But how do we actually use data to inform decision-making? Sometimes how we use the data is obvious, but oftentimes we want to learn not just about the data (which we observe) but what generated the data (which we don’t observe). We can observe data from a sample of respondents, but how can we use that to learn something about the entire population of respondents? We can observe the choices that people make, but how can we use that to learn something about the process that leads them to make those choices? To understand this unobserved, data-generating process, we need a model (i.e., a likelihood).

Prior to observing data, we have beliefs, a probability distribution over all possible outcomes. Data tells us something about how likely those possible outcomes are. The combination of our prior beliefs and the data (more explicitly, the likelihood of what we observe) results in a posterior probability distribution over the possible outcomes. Note that this is still a probability distribution — again, we have not removed uncertainty — but one that has now been informed by data. We can use this posterior distribution to help us make decisions in the presence of uncertainty.

While the language is new, the process should be intuitive:

1. We have beliefs about the probability of certain outcomes.
2. We observe outcomes.
3. We update our beliefs about the probability of these outcomes.

This updating process is learning! This is how we learn. And that’s all statistical inference is – a formal way to update our beliefs/learn/quantity uncertainty.

Example here to solidify the intuition, introduce Bayes’ theorem.

### Models and parameters

Now we need to get into the details about these three components: the prior, the likelihood, and the posterior.

The prior is a probability distribution. If we don’t know anything, we can assume a uniform distribution (or, more generally, something uninformative). However, we usually know something (even if we aren’t really certain about it), and the prior provides us with a formal way to include that knowledge. To connect this to learning, the prior could be the posterior from a previous analysis. Thus, we can see statistical inference as a way to formalize learning not just once but as it really occurs: in sequence, over time.

The likelihood is a story that describes where the data come from. This begins conceptually and then is formalized into a model that tell us the probability of any possible observation. This conceptual model often lives within a literature of model building, motivated by theory, and at its most basic may simply be a consideration of how to relax assumptions in an existing model. In short, the model needs to be consistent with our domain expertise. How do we know we have the right model? The truth is, we don’t. We don’t observe the data generating process. The best we can do is to create new models and compare them. This is science – the endless process of developing and refining theory. The truth is, for most things, simple models work well and there are standard models we’ll start with. However, there are common models that we work with that are best for many situations.

Example of a likelihood and counting (see Statistical Rethinking p. 27, 32); introduce motivations for using normal distributions.

The posterior is the prior distribution, updated according to how likely the data we observe are given our model.

Show how data can overwhelm the prior.

### Frequentism as a special case

Illustrating the difference. Frequentist inference always feels like backing into a problem, but how to illustrate this idea in a way that is intuitive?

| Concepts | Bayesian Inference | Frequentist Inference |
|------------------------|------------------------|------------------------|
| Random variables vs. constants/fixed variables | All unknowns (parameters, missing data, etc.) are random variables, data is fixed | Data is a random variable, parameters are fixed |
| Probability distributions | Posterior distributions | Sampling distributions |
|  |  |  |
|  |  |  |
|  |  |  |
|  |  |  |
|  |  |  |
|  |  |  |
|  |  |  |
|  |  |  |

## Decision theory

We live in a world filled with uncertainty. Will I pass a test? Will it rain tomorrow? Especially as we make decisions, we want to use data to reduce uncertainty. In other words, we use data to make informed decisions. Note that data reduces rather than removes uncertainty. How I perform on the practice test doesn’t guarantee how I’ll perform on the actual exam. The weather this morning tells me something about what I might expect this afternoon, but it’s not a perfect prediction. Even when I use data to inform my decision-making, I will still be making decisions in the presence of uncertainty.

## Final thoughts

Final thoughts.
