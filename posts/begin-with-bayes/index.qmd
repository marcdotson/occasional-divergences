---
title: "Begin with Bayes"
author: "Marc Dotson"
date: "2024-12-31"
description: |
  Begin with Bayes.
categories:
  - bayesian inference
  - pml
image: figures/positron-logo.png
slug: begin-with-bayes
---

https://www.the100.ci/2024/08/27/lets-do-statistics-the-other-way-around/

https://elevanth.org/blog/2021/06/15/regression-fire-and-dangerous-things-1-3/

https://www.tandfonline.com/doi/full/10.1080/29932955.2024.2370620

Introduction to the idea of facilitating statistical literacy by beginning with a unifying probabilistic lense that leads into Bayesian inference. Collect resources and links. A teaching philosophy. The beginnings of a pedagogical paper.

Can I compare frequentism to Trinitarian ideas? Not intuitive but hard to let go of once they take root.

-   Reference the \[blog post\](<http://varianceexplained.org/r/teach-tidyverse/>) that first inspired this idea.
-   Transfer notes and reference from \[this old issue\](<https://github.com/marcdotson/old-blog/issues/20>) and what I have in Notes.
-   A need to build intuition for non-technical students.
-   The necessity of intuition having a "unifying principle" to build on.
-   Who said that Bayesian inference is what happens when you take probability seriously?
-   How to introduce Bayes without using frequentist language (McElreath).
-   Not treating Bayesian inference as an advanced topic that comes after frequentist inference.

## Bayes is dead. Long live Bayes.

*Regression and Other Stories* is frustrating. It employs Bayesian concepts early, but presents them using the language of frequentist inference. This is likely applicable to a certain audience already well versed in frequentist language, like using Catholic terminology to explain the Restoration. However, it conflates the probabilistic foundations that give rise to Bayesian inference as a natural extension.

*Probabilistic Machine Learning* provides a genuinely unifying principle by incorporating all of statistics and machine learning within the language and probability. Only from a probabilistic perspective can we see Bayesian inference as a natural extension, without the baggage of making the Bayesian approach fit into frequentist terminology, as if frequentist terminology were the truth. This is for a difference audience, once that isn't committed to or have an understanding of the frequentist approach.

I'm assuming *An Introduction to Statistical Learning* takes another tack, and tries to unify statistical and machine learning concepts from a decidedly ML perspective.

## Frequentism as a special case

Illustrating the difference. Frequentist inference always feels like backing into a problem, but how to illustrate this idea in a way that is intuitive?

| Concepts | Bayesian Inference | Frequentist Inference |
|------------------------|------------------------|------------------------|
| Random variables vs. constants/fixed variables | All unknowns (parameters, missing data, etc.) are random variables, data is fixed | Data is a random variable, parameters are fixed |
| Probability distributions | Posterior distributions | Sampling distributions |
|  |  |  |
|  |  |  |
|  |  |  |
|  |  |  |
|  |  |  |
|  |  |  |
|  |  |  |
|  |  |  |

## Final thoughts

Final thoughts.