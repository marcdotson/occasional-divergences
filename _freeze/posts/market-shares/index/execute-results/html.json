{
  "hash": "0882adf313b8c538989bc14c1a44e898",
  "result": {
    "markdown": "---\ntitle: \"Estimating market shares and causal estimands from conjoint data\"\nauthor: \"Marc Dotson\"\ndate: \"2023-08-30\"\ndescription: |\n  Posterior predictions resulting from modeling conjoint experiment data yield a number of informative statistics, though in practice there hasn't been as much concern with making formal causal estimates.\ncategories:\n  - choice models\n  - bayesian inference\n  - posterior predictions\n  - causal inference\n  - stan\n  - r\nimage: Figures/flat-regression-contrasts-dummy-01.png\nslug: market-shares\n---\n\n\nPosterior predictions resulting from modeling conjoint experiment data yield a number of informative statistics, though in practice there hasn't been as much concern with making formal causal estimates. However, there is a relationship between market shares and computing causal estimates.\n\nIn this blog we explore how market shares and causal estimates are computed \"in practice\" and consider a new way to compute market shares that retains all of the benefits found in the causal estimates, including uncertainty estimates for market shares. We call these \"causal market shares.\"\n\n## Andrew's post about different kinds of posterior predictions\n\n[https://www.andrewheiss.com/blog/2022/09/26/guide-visualizing-types-posteriors](link)\n\n### Posterior predictions\n\n`tidybayes::add_predicted_draws()` and `brms::posterior_predict()` adds draws from the posterior predictive distribution. In other words, given the posterior, what is the predicted distribution of outcomes. This posterior predictive distribution propagates all uncertainty in the posterior distribution through to the distribution of outcomes (i.e., it uses the entire distribution of posterior draws to make the predictive distribution).\n\n### Expectation of the posterior predictive distribution\n\n`tidybayes::add_epred_draws()` and `brms::posterior_epred()` adds draws from the *expectation* of the posterior predictive distribution. This assumes that the posterior predictive distribution has an expectation. This is analogous to taking the expectation (e.g., mean or median) of a marginal posterior distribution to describe the most likely parameter value. Instead, we're using the expectation of the posterior predictive distribution to describe the most likely predicted outcome.\n\nAs with the posterior predictive distribution, this propagates all uncertainty forward but then abandons it for the expected value only. Additionally, the uncertainty present in the variance is no longer included.\n\n### Posterior of the linear predictor\n\n`tidybayes::add_linpred_draws()` and `brms::posterior_linpred()` adds draws the posterior linear predictors. This is a posterior prediction for the linear model only. It's possible that the posterior linear predictor is equivalent to the expectation of the posterior predictive distribution (e.g., with Normal regression).\n\nAdditionally, the uncertainty present in the variance is no longer included.\n\n## Andrew's post about ACMEs and marginal means\n\n[https://www.andrewheiss.com/blog/2023/07/25/conjoint-bayesian-frequentist-guide/#average-marginal-component-effect-amce](link)\n\nIs using point estimates to compute market shares the same as the expectation of the posterior predictive distribution? If not, how is it different? I suppose there is no uncertainty present in the prediction? Is it because the uncertainty present in the variance is no longer included?\n\n*Causal estimates are some sort of averaged probability conditioned on counterfactual predictions.*\n\naverage of `gammas_draw %*% every possible combination of levels` for every draw...\n\n1. Take the dot product of the parameter estimates and *every* possible combination of the attribute levels. Do this for *every* (post warm-up) draw from the posterior.\n2. Take the inverse logit of those products to convert them to the probability space.\n3. Average these predicted probabilities across draws for a given attributes' levels.\n4. Produce any contrast you'd like using these (I don't know what to call them) marginal posterior predictive counterfactual distributions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# see acme_mms.R in why-donors-donate/private\n\n# This is like computing the persona betas in the market simulator:\n# - Instead of organization or persona configurations, we have all possible combinations of attribute levels.\n# - Instead of computing expected utility, we're averaging to marginal means.\n# - Instead of normalizing to market shares, we're using plogis() to get everything on probability scales.\n# - And we're doing this for the entire set of posterior draws.\n\n###################################################################\n# Computing marginal means on the probability scale.\n###################################################################\n# Start with the Gamma draws form the heterogeneity model.\nall_combos_means <- gammas_raw %>%\n  # Get a data frame of all estimable attribute levels.\n  pivot_wider(names_from = i, values_from = Gamma) %>%\n  # Create a list-column of draws for all estimable attribute levels.\n  group_by(j, .chain, .iteration, .draw) %>%\n  nest() %>%\n  # Matrix multiply each set of draws by the all combinations grid.\n  mutate(pi = map(data, ~{\n    preds <- as.numeric(all_combos_design_grid %*% as.numeric(.))\n    all_combos %>%\n      mutate(\n        .linpred = preds,\n        # fake bc it's not actually mean(posterior_predict(.))\n        .epred = plogis(preds)\n      )\n  })) %>%\n  select(-data) %>%\n  unnest(pi)\n\n# Let's split this up into parts...\n###################################################################\ngammas_nested <- gammas_raw %>%\n  # Get a data frame of all estimable attribute levels.\n  pivot_wider(names_from = i, values_from = Gamma) %>%\n  # Create a list-column of draws for all estimable attribute levels.\n  group_by(j, .chain, .iteration, .draw) %>%\n  nest()\n\n# Single draw of gammas.\ngammas_unnested <- gammas_nested |> ungroup() |> slice(1) |> select(data) |> unnest(cols = c(data)) |> as.numeric()\n\n# Matrix multiply single draw of gammas with every possible combination.\npreds <- as.numeric(all_combos_design_grid %*% gammas_unnested)\n\n# Append the dot product for every possible combination with a single draw of gammas.\nall_combos_one_draw <- all_combos %>%\n  mutate(\n    .linpred = preds,\n    # fake bc it's not actually mean(posterior_predict(.))\n    .epred = plogis(preds)\n  )\n\n# Now stack this with the same procedure for *every* draw.\nall_combos_one_draw\n###################################################################\n```\n:::\n\n\n## Market shares\n\n- Market shares using point estimates.\n- Market shares propagating uncertainty from the entire posterior.\n- Place of different kind of posterior predictions in market share calculations?\n\n## Causal estimands\n\nWhat are the posterior predictions for causal inference?\n- Use {marginaleffects}?\n- Standardization/g-computation, inverse probability weighting, and maximum likelihood estimation.\n- Application of causal machine learning?\n\nCan we think about ACMEs as a different way for computing attribute or level importance?\n\n## Equivalence\n\n### Using posterior means first\n\n### Propagation uncertatiny from the posterior\n\n## Final thoughts\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}