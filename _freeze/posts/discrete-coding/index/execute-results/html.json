{
  "hash": "0c94a81b139ed697b1d9cfdd2f67cb3b",
  "result": {
    "markdown": "---\ntitle: \"Coding discrete explanatory variables\"\nauthor: \"Marc Dotson\"\ndate: \"2023-01-10\"\ndescription: |\n  There are a number of ways to code discrete explanatory variables. In this post, we explore the equivalence of using two of the most common ways: dummy coding and index coding.\ncategories:\n  - index coding\n  - bayesian inference\n  - stan\n  - hierarchical models\n  - choice models\n  - r\nimage: Figures/flat-contrasts-dummy-01.png\nslug: discrete-coding\n---\n\n\nThere are a number of ways to code (i.e., encode) discrete (i.e., categorical) explanatory variables, with different coding strategies suited for specific use cases. In this post, we explore using two of the most common ways to code discrete explanatory variables: dummy coding and index coding. Another common approach is called effects or sum-to-one coding. For a great walkthrough of that approach and its benefits in the context of choice modeling, see Elea Feit's [post](https://eleafeit.com/posts/2021-05-23-parameterization-of-multinomial-logit-models-in-stan/) that helped inspire this one.\n\nLet's start with a simple flat regression and then move to a far more complicated hierarchical multinomial logit.\n\n## Flat regression\n\nLet's generate flat regression data that includes discrete explanatory variables.\n\n\n::: {.cell output.var='generate_flat_regression_data'}\n\n```{.stan .cell-code}\n// Index and parameter values.\ndata {\n  int<lower = 1> N;    // Number of observations.\n  int<lower = 1> I;    // Number of covariates.\n  matrix[N, I] X;      // Matrix of covariates.\n\n  vector[I] beta;      // Vector of slopes.\n  real<lower = 0> tau; // Variance of the regression.\n}\n\n// Generate data according to the flat regression.\ngenerated quantities {\n  // Vector of observations.\n  vector[N] y;\n\n  // Generate data.\n  for (n in 1:N) {\n    y[n] = normal_rng(X[n,] * beta, tau);\n  }\n}\n```\n:::\n\n\nNow we create the matrix of covariates and call `generate_flat_regression_data`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages.\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(tidybayes)\n\n# Set the simulation seed.\nset.seed(42)\n\n# Specify data and parameter values.\nsim_values <- list(\n  N = 50,                    # Number of observations.\n  I = 5,                     # Number of covariates.\n  J = c(2, 3),               # Number of levels for each discrete variable.\n  beta = c(1, -4, 6, 3, -2), # Vector of slopes.\n  tau = 1                    # Variance of the regression.\n)\n\n# Matrix of covariates.\nsim_X <- matrix(data = 0, nrow = sim_values$N, ncol = (sim_values$I))\nfor (n in 1:sim_values$N) {\n  temp_X <- NULL\n  for (j in 1:length(sim_values$J)) {\n    temp_J <- rep(0, sim_values$J[j])\n    temp_J[sample(seq(1, (sim_values$J[j])), 1)] <- 1\n    temp_X <- c(temp_X, temp_J)\n  }\n  sim_X[n,] <- temp_X\n}\nsim_values$X <- sim_X\n\nhead(sim_values$X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    0    1    0    0\n[2,]    1    0    1    0    0\n[3,]    0    1    0    1    0\n[4,]    0    1    1    0    0\n[5,]    0    1    0    0    1\n[6,]    0    1    0    0    1\n```\n:::\n\n```{.r .cell-code}\n# Compile the model for generating data.\ngenerate_flat_regression_data <- cmdstan_model(\n  stan_file = here::here(\"posts\", \"discrete-coding\", \"Code\", \"generate_flat_regression_data.stan\"),\n  dir = here::here(\"posts\", \"discrete-coding\", \"Code\", \"Compiled\")\n)\n\n# Generate data.\nsim_data <- generate_flat_regression_data$sample(\n  data = sim_values,\n  chains = 1,\n  iter_sampling = 1,\n  seed = 42,\n  fixed_param = TRUE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 1 chain...\n\nChain 1 Iteration: 1 / 1 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\n```\n:::\n\n```{.r .cell-code}\n# Extract generated data.\nsim_y <- sim_data$draws(variables = \"y\", format = \"draws_list\") %>%\n  pluck(1) %>%\n  flatten_dbl()\n\nhead(sim_y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  6.927270  8.074890 -0.379599  2.647710 -4.816130 -5.506060\n```\n:::\n:::\n\n\n### Dummy coding\n\nAlso known as indicator coding, dummy coding is the most common way to deal with discrete variables, where a single discrete variable with `K` levels is encoded as `K - 1` binary columns, each indicating the presence or absence of the given level. It is an approach to identifying the estimates of discrete explanatory levels that is inherited from frequentist models.\n\nIf we include all levels of a single discrete variable, they sum up *across columns* to a constant---to an *intercept*. If we did that with more than one discrete variable, we would have more than one intercept and they would no longer be identified. With dummy coding, it is typical to include an intercept (i.e., a constant, often a column of `1`'s) and drop the first level (i.e., the reference level) of each of the discrete variables.\n\nHere's a flat regression using dummy coding.\n\n\n::: {.cell output.var='flat_regression_dummy'}\n\n```{.stan .cell-code}\n// Index value and observations.\ndata {\n  int<lower = 1> N;    // Number of observations.\n  int<lower = 1> I;    // Number of covariates.\n  vector[N] y;         // Vector of observations.\n  matrix[N, I] X;      // Matrix of covariates.\n}\n\n// Parameters.\nparameters {\n  real alpha;          // Intercept.\n  vector[I] beta;      // Vector of slopes.\n  real<lower = 0> tau; // Variance of the regression.\n}\n\n// Regression.\nmodel {\n  // Priors.\n  alpha ~ normal(0, 5);\n  for (i in 1:I) {\n    beta[i] ~ normal(0, 5);\n  }\n  tau ~ normal(0, 5);\n\n  // Likelihood.\n  for (n in 1:N) {\n    y[n] ~ normal(alpha + X[n,] * beta, tau);\n  }\n}\n```\n:::\n\n\nLet's run this flat regression using dummy coding.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specify data.\ndata <- list(\n  N = length(sim_y),    # Number of observations.\n  I = ncol(sim_X) - 2,  # Number of covariates.\n  y = sim_y,            # Vector of observations.\n  X = sim_X[, -c(1, 3)] # Matrix of covariates.\n)\n\n# Compile the model.\nflat_regression_dummy <- cmdstan_model(\n  stan_file = here::here(\"posts\", \"discrete-coding\", \"Code\", \"flat_regression_dummy.stan\"),\n  dir = here::here(\"posts\", \"discrete-coding\", \"Code\", \"Compiled\")\n)\n\n# Fit the model.\nfit_dummy <- flat_regression_dummy$sample(\n  data = data,\n  chains = 4,\n  parallel_chains = 4,\n  seed = 42\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.2 seconds.\nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.2 seconds.\nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.2 seconds.\nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.4 seconds.\n```\n:::\n:::\n\n\nNote that we can't directly recover the parameters values we used when simulating the data. Dummy coding is equivalent to specifying each included level as a contrast with the reference level. We can compare the dummy-coded marginal posteriors to the contrasted true parameter values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract draws and compare contrasts.\ncontrast_values <- tibble(\n  .variable = str_c(\"contrast\", 1:(sim_values$I - length(sim_values$J))),\n  values = c(\n    # First discrete variable.\n    sim_values$beta[2] - sim_values$beta[1],\n    # Second discrete variable.\n    sim_values$beta[4] - sim_values$beta[3],\n    sim_values$beta[5] - sim_values$beta[3]\n  )\n)\nfit_dummy$draws(variables = \"beta\", format = \"draws_df\") %>%\n  mutate_variables(\n    contrast1 = `beta[1]`,\n    contrast2 = `beta[2]`,\n    contrast3 = `beta[3]`\n  ) %>%\n  gather_draws(contrast1, contrast2, contrast3) %>%\n  ggplot(aes(y = .variable, x = .value)) +\n  stat_histinterval() +\n  geom_vline(aes(xintercept = values), contrast_values, color = \"red\") +\n  facet_wrap(~ .variable, scales = \"free\", ncol = 1)\n```\n:::\n\n\n![](Figures/flat-regression-contrasts-dummy-01.png){width=750px}\n\nThe drawback to dummy coding in a Bayesian setting with *real data* is that we need to specify priors over the contrasts rather than the parameters themselves. This complication for setting priors is a good reason to consider effects coding (again, see Elea Feit's [post](https://eleafeit.com/posts/2021-05-23-parameterization-of-multinomial-logit-models-in-stan/) on that topic). Dummy coding also enshrines the reference level as something special, making it non-trivial to compute other contrasts.\n\n### Index coding\n\nAlso known as one-hot encoding, index coding similarly turns each level of a discrete variable into its own binary column. However, with index coding we *don't* include an intercept and *don't* include any reference levels.\n\nBy not including reference levels, the intercept is implied by the fact that the columns sum to a constant, as discussed previously. But when we have more than one discrete variable we also have more than one implied intercept. This would create an identification problem in a frequentist setting, but in a Bayesian analysis we simply rely on the prior to enable identification of each of the parameters. As a bonus, the contrasts are always identified even if their constituent parameter estimates are not.\n\nHere's a flat regression using index coding. Note that the only difference here is the absence of the intercept `alpha`.\n\n\n::: {.cell output.var='flat_regression_index'}\n\n```{.stan .cell-code}\n// Index value and observations.\ndata {\n  int<lower = 1> N;    // Number of observations.\n  int<lower = 1> I;    // Number of covariates.\n  vector[N] y;         // Vector of observations.\n  matrix[N, I] X;      // Matrix of covariates.\n}\n\n// Parameters.\nparameters {\n  vector[I] beta;      // Vector of slopes.\n  real<lower = 0> tau; // Variance of the regression.\n}\n\n// Regression.\nmodel {\n  // Priors.\n  for (i in 1:I) {\n    beta[i] ~ normal(0, 5);\n  }\n  tau ~ normal(0, 5);\n\n  // Likelihood.\n  for (n in 1:N) {\n    y[n] ~ normal(X[n,] * beta, tau);\n  }\n}\n```\n:::\n\n\nAnd now let's run this flat regression using index coding.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specify data.\ndata <- list(\n  N = length(sim_y),   # Number of observations.\n  I = ncol(sim_X),     # Number of covariates.\n  y = sim_y,           # Vector of observations.\n  X = sim_X            # Matrix of covariates.\n)\n\n# Compile the model.\nflat_regression_index <- cmdstan_model(\n  stan_file = here::here(\"posts\", \"discrete-coding\", \"Code\", \"flat_regression_index.stan\"),\n  dir = here::here(\"posts\", \"discrete-coding\", \"Code\", \"Compiled\")\n)\n\n# Fit the model.\nfit_index <- flat_regression_index$sample(\n  data = data,\n  chains = 4,\n  parallel_chains = 4,\n  seed = 42\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 finished in 0.4 seconds.\nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.4 seconds.\nChain 3 finished in 0.4 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 0.6 seconds.\n```\n:::\n:::\n\n\nWith index coding we *can* directly recover the parameters values we used when simulating the data! Additionally, it is straightforward to produce whatever contrasts we'd like! Here we re-produce the same plots as before and compare it to the contrasted true parameter values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract draws and compare contrasts.\ncontrast_values <- tibble(\n  .variable = str_c(\"contrast\", 1:(sim_values$I - length(sim_values$J))),\n  values = c(\n    # First discrete variable.\n    sim_values$beta[2] - sim_values$beta[1],\n    # Second discrete variable.\n    sim_values$beta[4] - sim_values$beta[3],\n    sim_values$beta[5] - sim_values$beta[3]\n  )\n)\nfit_index$draws(variables = c(\"beta\"), format = \"draws_df\") %>%\n  mutate_variables(\n    contrast1 = `beta[2]` - `beta[1]`,\n    contrast2 = `beta[4]` - `beta[3]`,\n    contrast3 = `beta[5]` - `beta[3]`\n  ) %>%\n  gather_draws(contrast1, contrast2, contrast3) %>%\n  ggplot(aes(y = .variable, x = .value)) +\n  stat_histinterval() +\n  geom_vline(aes(xintercept = values), contrast_values, color = \"red\") +\n  facet_wrap(~ .variable, scales = \"free\", ncol = 1)\n```\n:::\n\n\n![](Figures/flat-regression-contrasts-index-01.png){width=750px}\n\nThe contrasts and the recovered parameters are the same as the results from the dummy-coded model. Index coding like this is only possible in a Bayesian framework where the use of priors obviates the necessity of strict identification strategies. However, it's not without costs. We will need to specify informative priors, which we should be doing anyway, and the model itself may take longer to converge.\n\n## Hierarchical multinomial logit\n\nA hierarchical multinomial logit is the standard for choice modeling in marketing (see previous posts for details on [hierarchical models](https://occasionaldivergences.com/posts/stan-hierarchical/) and [choice models](https://occasionaldivergences.com/posts/choice-models/)). We'll specifically be using a [non-centered parameterization](https://occasionaldivergences.com/posts/non-centered/).\n\nFirst, let's generate data with discrete explanatory variables from a hierarchical multinomial logit.\n\n\n::: {.cell output.var='generate_hier_mnl_data'}\n\n```{.stan .cell-code}\n// Index and hyperprior values.\ndata {\n  int<lower = 1> R;           // Number of respondents.\n  int<lower = 1> S;           // Number of choice tasks.\n  int<lower = 2> A;           // Number of choice alternatives.\n  int<lower = 1> I;           // Number of observation-level covariates.\n  int<lower = 1> J;           // Number of population-level covariates.\n\n  real Gamma_mean;            // Mean of population-level means.\n  real<lower=0> Gamma_scale;  // Scale of population-level means.\n  real<lower=0> Omega_shape;  // Shape of population-level scale.\n  real tau_df;                // Degrees of freedom of population-level scale.\n  \n  array[R, S] matrix[A, I] X; // Array of observation-level covariates.\n  matrix[R, J] Z;             // Matrix of population-level covariates.\n}\n\n// Generate data according to the hierarchical multinomial logit.\ngenerated quantities {\n  matrix[R, S] Y;             // Matrix of observations.\n  matrix[J, I] Gamma;         // Matrix of population-level hyperparameters.\n  corr_matrix[I] Omega;       // Population model correlation matrix hyperparameters.\n  vector[I] tau;              // Population model vector of scale hyperparameters.\n  matrix[R, I] Beta;          // Matrix of observation-level parameters.\n\n  // Draw parameter values and generate data.\n  for (j in 1:J) {\n    for (i in 1:I) {\n      Gamma[j, i] = normal_rng(Gamma_mean, Gamma_scale);\n    }\n  }\n  for (i in 1:I) {\n    tau[i] = chi_square_rng(tau_df);\n  }\n  Omega = lkj_corr_rng(I, Omega_shape);\n  for (r in 1:R) {\n    Beta[r,] = multi_normal_rng(Z[r,] * Gamma, quad_form_diag(Omega, tau))';\n    for (s in 1:S) {\n      Y[r, s] = categorical_logit_rng(X[r, s] * Beta[r,]');\n    }\n  }\n}\n```\n:::\n\n\nNow we create covariate arrays and call `generate_hier_mnl_data`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set the simulation seed.\nset.seed(42)\n\n# Specify data and hyperprior values.\nsim_values <- list(\n  R = 500,           # Number of respondents.\n  S = 10,            # Number of choice tasks.\n  A = 4,             # Number of choice alternatives.\n  L = c(3, 4, 5),    # Number of levels for each discrete attribute.\n  I = 12,            # Number of observation-level covariates.\n  J = 3,             # Number of population-level covariates.\n  \n  Gamma_mean = 0,    # Mean of population-level means.\n  Gamma_scale = 5,   # Scale of population-level means.\n  Omega_shape = 2,   # Shape of population-level scale.\n  tau_df = 2         # Degrees of freedom of population-level scale.\n)\n\n# Array of observation-level covariates.\nsim_X <- array(\n  data = NA,\n  dim = c(sim_values$R, sim_values$S, sim_values$A, sim_values$I)\n)\nfor (r in 1:sim_values$R) {\n  for (s in 1:sim_values$S) {\n    temp_S <- NULL\n    for (l in 1:length(sim_values$L)) {\n      temp_L <- NULL\n      for (a in 1:sim_values$A) {\n        temp_A <- matrix(0, nrow = 1, ncol = sim_values$L[l])\n        temp_A[1, sample(seq(1, sim_values$L[l]), 1)] <- 1\n        temp_L <- rbind(temp_L, temp_A)\n      }\n      temp_S <- cbind(temp_S, temp_L)\n    }\n    sim_X[r, s,,] <- temp_S\n  }\n}\nsim_values$X <- sim_X\n\nhead(sim_X[1,1,,])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n[1,]    1    0    0    0    1    0    0    1    0     0     0     0\n[2,]    1    0    0    0    0    0    1    0    0     0     1     0\n[3,]    1    0    0    0    1    0    0    1    0     0     0     0\n[4,]    1    0    0    0    1    0    0    0    0     0     0     1\n```\n:::\n\n```{.r .cell-code}\n# Matrix of population-level covariates.\nsim_Z <- cbind(\n  rep(1, sim_values$R),\n  matrix(\n    runif(sim_values$R * (sim_values$J - 1), min = 2, max = 5),\n    nrow = sim_values$R\n  )\n)\nsim_values$Z <- sim_Z\n\nhead(sim_Z)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]     [,2]     [,3]\n[1,]    1 3.040355 2.326080\n[2,]    1 2.367725 3.940835\n[3,]    1 3.905701 4.309898\n[4,]    1 2.535441 4.453046\n[5,]    1 4.853128 2.703989\n[6,]    1 4.946704 3.327675\n```\n:::\n\n```{.r .cell-code}\n# Compile the model for generating data.\ngenerate_hier_mnl_data <- cmdstan_model(\n  stan_file = here::here(\"posts\", \"discrete-coding\", \"Code\", \"generate_hier_mnl_data.stan\"),\n  dir = here::here(\"posts\", \"discrete-coding\", \"Code\", \"Compiled\")\n)\n\n# Generate data.\nsim_data <- generate_hier_mnl_data$sample(\n  data = sim_values,\n  chains = 1,\n  iter_sampling = 1,\n  seed = 42,\n  fixed_param = TRUE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 1 chain...\n\nChain 1 Iteration: 1 / 1 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\n```\n:::\n\n```{.r .cell-code}\n# Extract generated data.\nsim_Y <- sim_data$draws(variables = \"Y\", format = \"draws_list\") %>%\n  pluck(1) %>%\n  flatten_dbl()\n\nhead(sim_Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2 4 1 2 2 2\n```\n:::\n:::\n\n\n## Dummy coding\n\n- Update extracting generated data for an HMNL above.\n- Compare this `hmnl_noncentered` to what is used for why-donors-donate (hmnl_parameterizations.R?)\n- Discuss specifying a brand intercept (i.e., index coding the brand attribute) in a hierarchical multinomial logit model where an intercept isn't identified?\n\nIf intercepts aren't identified, it could be the prior alone that is providing a sensible posterior. To test this, let's use a uniform prior. If it's not just the prior, then when we consider choice models, it's the outside good that is at play. The outside good provides a reference to the alternatives, a kind of intercept that is identified in the model even when a traditional intercept is not. To test, include the outside good coded as zero with brand intercepts and the outside good coded as a binary attribute (it's own intercept) with dummy coded brands. These should be equivalent. But what if it's a forced choice and there is no outside option? Did we include a forced choice for why-donors-donate?\n\n---\n\nAlso known as indicator coding, dummy coding is the most common way to deal with discrete variables, where a single discrete variable with `K` levels is encoded as `K - 1` binary columns, each indicating the presence or absence of the given level. It is an approach to identifying the estimates of discrete explanatory levels that is inherited from frequentist models.\n\nIf we include all levels of a single discrete variable, they sum up *across columns* to a constant---to an *intercept*. If we did that with more than one discrete variable, we would have more than one intercept and they would no longer be identified. With dummy coding, it is typical to include an intercept (i.e., a constant, often a column of `1`'s) and drop the first level (i.e., the reference level) of each of the discrete variables.\n\nHere's a flat regression using dummy coding.\n\n\n::: {.cell output.var='hier_mnl_dummy'}\n\n```{.stan .cell-code}\n\n```\n:::\n\n\nLet's run this flat regression using dummy coding.\n\n\n::: {.cell}\n\n:::\n\n\nNote that we can't directly recover the parameters values we used when simulating the data. Dummy coding is equivalent to specifying each included level as a contrast with the reference level. We can compare the dummy-coded marginal posteriors to the contrasted true parameter values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract draws and compare contrasts.\ncontrast_values <- tibble(\n  .variable = str_c(\"contrast\", 1:(sim_values$I - length(sim_values$J))),\n  values = c(\n    # First discrete variable.\n    sim_values$beta[2] - sim_values$beta[1],\n    # Second discrete variable.\n    sim_values$beta[4] - sim_values$beta[3],\n    sim_values$beta[5] - sim_values$beta[3]\n  )\n)\nfit_dummy$draws(variables = \"beta\", format = \"draws_df\") %>%\n  mutate_variables(\n    contrast1 = `beta[1]`,\n    contrast2 = `beta[2]`,\n    contrast3 = `beta[3]`\n  ) %>%\n  gather_draws(contrast1, contrast2, contrast3) %>%\n  ggplot(aes(y = .variable, x = .value)) +\n  stat_histinterval() +\n  geom_vline(aes(xintercept = values), contrast_values, color = \"red\") +\n  facet_wrap(~ .variable, scales = \"free\", ncol = 1)\n```\n:::\n\n\n<!-- ![](Figures/){width=750px} -->\n\nThe drawback to dummy coding in a Bayesian setting with *real data* is that we need to specify priors over the contrasts rather than the parameters themselves. This complication for setting priors is a good reason to consider effects coding (again, see Elea Feit's [post](https://eleafeit.com/posts/2021-05-23-parameterization-of-multinomial-logit-models-in-stan/) on that topic). Dummy coding also enshrines the reference level as something special, making it non-trivial to compute other contrasts.\n\n## Index coding\n\n\n## Final thoughts\n\nIndex coding is great if you can do it and demonstrates another benefit of being Bayesian. Intuitive. A little more computational complexity.\n\n#### Marc Dotson\n\nMarc is an assistant professor of marketing at the BYU Marriott School of Business. He graduated with an MSc from The London School of Economics and Political Science in 2009 and a PhD from The Ohio State University in 2016. His research interests include Bayesian inference, predictive modeling, consumer preference heterogeneity, and unstructured data. Marc teaches marketing analytics. You can find him on [Twitter](https://twitter.com/marcdotson) and [GitHub](https://github.com/marcdotson).\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}