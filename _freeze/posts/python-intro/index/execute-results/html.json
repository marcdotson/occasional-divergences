{
  "hash": "6c6420b212cc5c42c8b71652dcd50a20",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"An introduction to Python for R users\"\nauthor: \"Marc Dotson\"\ndate: \"2025-03-14\"\ndescription: |\n  This introduction to Python assumes you know R, which is used as an analogy to explain the language within the domain of data analytics.\ncategories:\n  - python\n  - r\nimage: figures/python.png\nslug: python-intro\nhighlight-style: arrow\nexecute:\n  eval: false\n---\n\n\n\nIn classes and workshops over many years, I've taught data analytics using R. But in my new position, I teach data analytics using Python. This introduction to Python is for R users---primarily me, though I hope it proves useful to others as well.\n\nThere are incredible resources in this space, and I've drawn liberally from a number of them. As an overall introduction to Python, [*Python for Data Analysis*](https://wesmckinney.com/book/) is a go-to resource and the spiritual equivalent of [*R for Data Science*](https://r4ds.hadley.nz). I also really appreciate the work in [*Python and R for the Modern Data Scientist*](https://www.amazon.com/Python-Modern-Data-Scientist-Worlds/dp/1492093408), especially for the authors' clear espousing that this isn't an either/or situation---you can (and arguably *should*) use Python and R as complements.\n\n![](figures/python-r.png){width=60% fig-align=\"center\"}\n\nI am especially indebted to Emily Riederer's blog series beginning with [Python Rgonomics](https://emilyriederer.netlify.app/post/py-rgo-2025/) and subscribe to her philosophy of using tools in Python that are genuinely \"Pythonic\" while being consistent with the workflow and ergonomics of the best R has to offer. I am also grateful to extra help from [posit::conf](https://posit.co/conference/) workshop instructors and colleagues in my new position at [Utah State University](https://huntsman.usu.edu/dais/). Additional resources will be provided where relevant in each section.\n\n## Different mindsets\n\nWhen you start working with Python, it's essential that you approach it with the right mindset. R is a specialized language developed by statisticians for data analysis. Python is a *big* tent, a general programming language developed by computer scientists for many things, with only a small portion of it dedicated to data analysis.\n\nTo summarize some key differences:\n\n| Python | R | \n| --- | --- | \n| General language | Specialized language | \n| Developed by computer scientists | Developed by statisticians | \n| Object-oriented programming | Functional programming | \n| Obsessed with efficiency | Lazy about efficiency | \n| Obsessed with namespacing | Lazy about namespacing | \n| Small, medium, and large data | Small and medium data | \n| Machine learning and deep learning | Data wrangling and visualization | \n| Spacing is part of the syntax | Spacing is for convenience | \n| Indices start with 0 | Indices start with 1 | \n| No single authority | Dominated by Posit | \n| Jupyter Notebooks | R Markdown and Quarto | \n| Inconsistent (i.e., no tidyverse) | Consistency in the tidyverse | \n| \"Pythonistas\" and \"Pythonic\" code | \"R Users\" and \"Tidy\" code |\n\nWhile these are broad strokes, they help provide some context for how the two languages deviate in their approaches to common problems.\n\n## Functions, methods, and attributes\n\nWith the right mindset, it's easier to understand some of the things that Python is obsessed with that R simply isn't. The most important difference is that Python is an object-oriented programming language while R is all about functional programming. While *everything* in R is a function (for a typical user), using Python requires frequently keeping track of the difference between functions, methods, and attributes.\n\nFunctions in Python and R are equivalent, although functions in Python are typically namespaced with the library name or alias like `library.function()`. Note that we can, but often don't, similarly namespace R functions with `package::function()`. Methods are object-specific functions. In other words, methods are functions nested within object types and are namespaced with an object name of the given type as in `object.method()`. While it's possible to import a specific function such that we can call it without referencing it's library name or alias, we can never call a method without reference to an object name of the necessary type. In other words, we may see a `function()` like in R but we will always see a `.method()` with an object. One more bit of language -- just like packages in Python are typically referred to as libraries, function (and method) arguments are referred to as parameters.\n\nAttributes are object-specific features and are, like methods, namespaced with an object name of the give type as in `object.attribute`, but without any parentheses. For example, the dimensions of a NumPy array could be referenced with `array.size` while the equivalent in base R would be a function like `dim(array)`.\n\n## Installation and reproducible environments\n\nThe first (and perhaps the biggest) hurdle as we work to apply our new mindset is simply getting Python and our reproducible environment installed. This is a big departure from what we're used to in R, where there is one way to install R and we usually ignore our environment, let alone [make our R environments reproducible](https://occasionaldivergences.com/posts/rep-env/). Remember, Python is a *big* tent with lots of uses and, unsurprisingly, lots of ways to do everything I'm covering. However, from the perspective of someone coming from R and with a focus on data analytics, I recommend the following. \n\n### pyenv\n\nUnlike your experience with R, Python comes pre-installed on some operating systems. This version *should not be used* by anyone except the OS itself. For this and other reasons, you'll need the ability to maintain multiple versions of Python on the same computer. I recommend using [pyenv](https://github.com/pyenv/pyenv), a Python version management tool that is designed to be as *simple* as possible, though what constitutes \"simple\" is a matter of experience and your mileage may vary.\n\n::: {.callout-tip title=\"The Command Line\"}\nUsing pyenv will require you to use the command line (i.e., terminal or shell). Be patient and *take your time* walking carefully through the [installation instructions](https://github.com/pyenv/pyenv?tab=readme-ov-file#installation). Daniel Chen, who introduced me to pyenv, also has a [great write-up](https://chendaniely.github.io/python_setup/210-python_install.html) of the installation instructions that may be a bit easier to follow. A few things to help as you install:\n\n- The command line is the programming interface into your OS itself. You don't have to know everything about it to follow instructions.\n- When you get to the section [Set up your shell environment for pyenv](https://github.com/pyenv/pyenv?tab=readme-ov-file#b-set-up-your-shell-environment-for-pyenv), the instructions are different based on the *type* of command line. If you're on a Mac that's running macOS Catalina 10.15.7 or later, the terminal is Zsh. If you're using Linux, the shell is Bash (and you probably already know that). And if you're using Windows, you'll be using the [pyenv-win](https://github.com/pyenv-win/pyenv-win) fork with PowerShell.\n:::\n\nNow that you have pyenv installed, you can install and manage all the versions of Python on your computer. To see what versions you already have installed, on the command line, run `pyenv versions`. At first this is probably just the OS version. Note that if you've installed Python before pyenv, it won't be listed: `pyenv versions` will only list the OS version and any additional versions you install using pyenv. To see all the available versions of Python that you can install, run `pyenv install --list`. This can be overwhelming, but a good place to start is the latest version of Python that has a stable release (i.e., doesn't have a `-dev` tag). For example, to install Python 3.12.4, run `pyenv install 3.12.4`.\n\nRemember how you shouldn't use the OS version of Python? Set the global default version of Python that you'll use (leaving the OS to do it's own thing). For example, to set Python 3.12.4 as the global default version, run `pyenv global 3.12.4`. If you run `pyenv versions` again you should see an asterisk by the global default you specified.\n\n::: {.callout-important title=\"Positron IDE\"}\nAs you well know, an integrated development environment (IDE), outside of an open source language, is arguably your most important tool as a data analyst. There are many options, but I recommend [Positron](https://positron.posit.co), a next-generation data science IDE. Built by Posit on VS Code's [open source core](https://github.com/microsoft/vscode), Positron combines the multilingual extensibility of [VS Code](https://code.visualstudio.com/) with essential data tools common to language-specific IDEs.\n\nIf RStudio is too specific and VS Code is too general, you may find that Positron is [just right](https://occasionaldivergences.com/posts/positron-intro/) and becomes your only IDE for both Python and R.\n:::\n\nThere's a lot more that [pyenv can do](https://realpython.com/intro-to-pyenv/). After navigating to a project working directory, you can go to the terminal and run `pyenv local 3.12.4`, assuming you want Python 3.12.4 to be set as the default. This would create a `.python-version` file in that directory that specifies the version of Python to use for that project.\n\n### venv\n\nIf an environment is composed of the open source language(s) and libraries (including the library dependencies) used for a given project, what makes an environment reproducible is keeping track of which *version* of the open source language(s) and the libraries we're using for a given project. While \"keeping track\" could happen in many different ways, ideally we want to keep track of our environment such that it can be easily *reproduced* on another machine, by you (including future you) or someone else. You've already seen how to use pyenv to keep track of the Python version by creating a `.python-version` file, and I recommend using [venv](https://docs.python.org/3/library/venv.html) to keep track of library versions.\n\nJust like with R, all Python libraries are installed in a single, global library on your computer known as the *system library*. What we need is a *project library*. This helps highlight an important feature of reproducible environments: Each project will have its own project library and thus be *isolated*. If two projects use different versions of the same package, they won't conflict with each other because they'll each have their own project library. (Well, not *exactly*. Python employs a global cache to avoid having to install the same version of a given library more than once. The project library will reference the global cache. Note that if you've installed Python prior to using pyenv, you may have a global cache that is borked. You can run `pip cache purge` in the command line to start fresh.)\n\nThe venv library comes pre-installed with Python as the default reproducible (or *virtual*, hence the \"v\" in venv) environment management tool. It works in two steps. First, we must create a project library. Second, we keep track of the versions of the libraries we use. After navigating to a project working directory, you can go to the terminal and run `python -m venv .venv`. This would create a `/.venv` folder in the working directory that contained the project library or *virtual environment*. Whenever you install new libraries or decide to update the versions of libraries you use, run `pip freeze > requirements.txt` to update `requirements.txt`.\n\nIt might seem like a lot (and it honestly is), but it's something [we should be doing in R](https://occasionaldivergences.com/posts/rep-env/) as well. Along with a project's code, all someone would need is the `.python-version` and `requirements.txt` files to reproduce your code, including the environment it was designed to run in.\n\n::: {.callout-caution title=\"uv\"}\nThere's a new(er) kid on the block that has been receiving lots of attention: [uv](https://docs.astral.sh/uv/). It promises to serve as a replacement for pyenv for Python version management, venv for managing reproducible environments, and even pip for installing libraries. (As a bellweather, the newest release of [{reticulate}](https://posit.co/blog/reticulate-1-41/) employs uv in the background.) I'll likely migrate to it, but it doesn't yet appear to seamlessly interact with Positron, and the IDE comes first for me.\n:::\n\n## Data wrangling\n\nWhatever the language, the most common task we have for any data analysis is data wrangling (i.e., cleaning, munging, etc.). The [NumPy](https://numpy.org) library is more or less equivalent to what we see in base R, introducing arrays and efficient computation across arrays---but, perhaps surprisingly, not data frames. Data frames (or DataFrames as they are referred to in Python) came later with [pandas](https://pandas.pydata.org) (short for \"panel data\"). Still the most popular library for data wrangling in Python, pandas is built to supplement NumPy, with all of the syntax baggage. However, growing in popularity is [Polars](https://pola.rs) (an anagram of the query language it uses, OLAP, and the language it's built in, Rust or rs). Polars is something of an answer to pandas problems. Actually, free from any Python code at all (yes, you can [use it in R](https://github.com/pola-rs/r-polars)), it offers a glimpse at what the polyglot future might look like.\n\nMy take is that Polars provides a far more self-consistent data wrangling experience than pandas, reversing the trend that many experience when they come to Polars for the speed and stay for the syntax. To illustrate the tidyverse spiritual connections, if not the deeper roots in SQL syntax, there are a number of great side-by-side comparisons of [Polars and {dplyr}](https://blog.tidy-intelligence.com/posts/dplyr-vs-polars/). I'll illustrate a few common tasks in both Polars and {dplyr} and point out the differences to be mindful of.\n\n### Import data\n\nLibraries and modules (a kind of sub-library) are imported with commonly accepted aliases in order to shorten the namespace reference. For example, the Polars alias convention is `pl`. We're also importing the os library, which needs no alias, to write out a relative file path that will work for any user or operating system that has the same relative directory structure.\n\nWe also see the difference between the `pl.read_csv()` function and the `.shape` and `.columns` attributes.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport polars as pl\nimport os\n\ncustomer_data = pl.read_csv(os.path.join('posts', 'python-intro', 'data', 'customer_data.csv'))\ncustomer_data.shape\ncustomer_data.columns\n```\n:::\n\n\n\n```\n(10531, 13)\n['customer_id',\n 'birth_year',\n 'gender',\n 'income',\n 'credit',\n 'married',\n 'college_degree',\n 'region',\n 'state',\n 'star_rating',\n 'review_time',\n 'review_title',\n 'review_text']\n```\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ncustomer_data <- read_csv(here::here(\"posts\", \"python-intro\", \"data\", \"customer_data.csv\"))\nglimpse(customer_data)\n```\n:::\n\n\n\n```\nRows: 10,531\nColumns: 13\n$ customer_id    <dbl> 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, …\n$ birth_year     <dbl> 1971, 1970, 1988, 1984, 1987, 1994, 1968, 1994, 1958, …\n$ gender         <chr> \"Female\", \"Female\", \"Male\", \"Other\", \"Male\", \"Male\", \"…\n$ income         <dbl> 73000, 31000, 35000, 64000, 58000, 164000, 39000, 6900…\n$ credit         <dbl> 742.0827, 749.3514, 542.2399, 573.9358, 644.2439, 553.…\n$ married        <chr> \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No…\n$ college_degree <chr> \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"No…\n$ region         <chr> \"South\", \"West\", \"South\", \"Midwest\", \"West\", \"Midwest\"…\n$ state          <chr> \"DC\", \"WA\", \"AR\", \"MN\", \"HI\", \"MN\", \"MN\", \"KY\", \"NM\", …\n$ star_rating    <dbl> 4, NA, NA, NA, 5, 2, NA, 5, NA, 5, NA, 5, NA, NA, NA, …\n$ review_time    <chr> \"06 11, 2015\", NA, NA, NA, \"03 25, 2008\", \"06 7, 2013\"…\n$ review_title   <chr> \"Four Stars\", NA, NA, NA, \"Great Product!!\", \"Not at a…\n$ review_text    <chr> \"everything's fine\", NA, NA, NA, \"I looked all over th…\n```\n:::\n\n### Filter observations\n\nPolars DataFrames have methods that are similar to {dplyr} (e.g., `.filter()` and `filter()`). DataFrames are composed of columns called Series (equivalent to R's vectors). Note that unlike pandas DataFrames, Polars DataFrames don't have a row index.\n\nIn pandas, we would need to reference column names with `data['column_name']` (like base R's `data$column_name` or `data[\"column_name\"]`), but Polars allows for `pl.col('column_name')`. Yes, we use quotation marks for every column name. The `pl.col()` expression offers a helper-function-like consistency.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncustomer_data.filter(pl.col('college_degree') == 'Yes')\ncustomer_data.filter(pl.col('region') != 'West')\ncustomer_data.filter(pl.col('gender') != 'Female', pl.col('income') > 70000)\n```\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter(customer_data, college_degree == \"Yes\")\nfilter(customer_data, region != \"West\")\nfilter(customer_data, gender == \"Female\", income > 70000)\n```\n:::\n\n\n:::\n\n### Slice observations\n\nRemember that Python is zero-indexed. This is probably the most problematic (and very computer science-based) difference and why it's nice to avoid indexing if you can!\n\nFunction (and method) arguments are called parameters. Some parameters are positional that have to be specified in an *exact* position. Others are keyword or named parameters, which is most common in R.\n\nThe positional parameters for Polars' `.slice()` method are the start index and the slice length.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncustomer_data.slice(0, 5)\n```\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslice(customer_data, 1:5)\n```\n:::\n\n\n:::\n\n### Sort observations\n\nIt can be strange at first, but namespacing is critical. Remember that a function is preceded by the library name or alias (e.g., `pl.col()`), unless you've imported the specific function (e.g., `from polars import col`), while a method is preceded by an object name of a certain type (e.g., `customer_data.sort()`). Since object types are tied to libraries, the chain back to its corresponding library is always present, explicitly for functions and implicitly for methods.\n\nNote that its `True` and `False`, not `TRUE` and `FALSE` or `true` and `false`.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncustomer_data.sort(pl.col('birth_year'))\ncustomer_data.sort(pl.col('birth_year'), descending = True)\n```\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\narrange(customer_data, birth_year)\narrange(customer_data, desc(birth_year))\n```\n:::\n\n\n:::\n\n### Select variables\n\nUsing single square brackets `[ ]` creates a list. This is similar to creating a vector in R with `c()`. A list is a fundamental Python object type and can be turned into a Series.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncustomer_data.select(pl.col('region'), pl.col('review_text'))\ncustomer_data.select(pl.col(['region', 'review_text']))\n```\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nselect(customer_data, region, review_text)\n```\n:::\n\n\n:::\n\n### Create new variables\n\nPolars is actually a query language, like SQL. So it's not surprising to see methods with names that more closely mirror queries, like the `.with_columns()` method.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncustomer_data.with_columns(income = pl.col('income') / 1000)\n```\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmutate(customer_data, income = income / 1000)\n```\n:::\n\n\n:::\n\n### Join data frames\n\nMissing values are identified as `NaN`. Series types include `str`, `bytes` (binary), `float`, `bool`, and `int`.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nstore_transactions = pl.read_csv(os.path.join('posts', 'python-intro', 'data', 'store_transactions.csv'))\nstore_transactions.shape\nstore_transactions.columns\n\ncustomer_data.join(store_transactions, on='customer_id', how='left')\ncustomer_data.join(store_transactions, on='customer_id', how='inner')\n```\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstore_transactions <- read_csv(here::here(\"posts\", \"python-intro\", \"data\", \"store_transactions.csv\"))\nglimpse(store_transactions)\n\nleft_join(customer_data, store_transactions, join_by(customer_id))\ninner_join(customer_data, store_transactions, join_by(customer_id))\n```\n:::\n\n\n:::\n\n### Consecutive lines of code\n\nWhile possible with Python code generally, Polars embraces writing consecutive lines of code using *method chaining*. Note that:\n\n- The entire chain needs to be surrounded with `( )`\n- Each line *starts* with `.`\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(customer_data\n  .join(store_transactions, on='customer_id', how='left')\n  .filter(pl.col('region') == 'West', pl.col('feb_2005') == pl.col('feb_2005').max())\n  .with_columns(age = 2024 - pl.col('birth_year'))\n  .select(pl.col(['age', 'feb_2005']))\n  .sort(pl.col('age'), descending=True)\n  .slice(0, 1)\n)\n```\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |> \n  left_join(store_transactions, join_by(customer_id)) |> \n  filter(region == \"West\", feb_2005 == max(feb_2005)) |> \n  mutate(age = 2024 - birth_year) |> \n  select(age, feb_2005) |> \n  arrange(desc(age)) |> \n  slice(1)\n```\n:::\n\n\n:::\n\n### Summarize discrete data\n\nThe `.agg()` method stands for *aggregate*, which is exactly what `summarize()` does in R.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(customer_data\n  .group_by(pl.col('region'))\n  .agg(n = pl.len())\n)\n\n(customer_data\n  .group_by(pl.col(['region', 'college_degree']))\n  .agg(n = pl.len())\n)\n```\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |> \n  count(region)\n\ncustomer_data |> \n  count(region, college_degree)\n```\n:::\n\n\n:::\n\n### Summarize continuous data\n\nThis is a good example of where object-oriented programming requires a different mindset. You might think that there is a general `mean()` function like in R, but there isn't and you'd have to load a specific library and reference its namespace to use such a function. Instead, `.mean()` is a method for Polars Series and DataFrames.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(customer_data\n  .select(pl.col('income'))\n  .mean()\n)\n\n(customer_data\n  .select(pl.col(['income', 'credit']))\n  .mean()\n)\n```\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |>\n  summarize(avg_income = mean(income))\n\ncustomer_data |>\n  summarize(\n    avg_income = mean(income),\n    avg_credit = mean(credit)\n  )\n```\n:::\n\n\n:::\n\n### Summarize discrete and continuous data\n\nCombining `.agg()` with `.group_by()` is similarly powerful as in {dplyr}.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(customer_data\n  .group_by(pl.col(['gender', 'region']))\n  .agg(\n    n = pl.len(), \n    avg_income = pl.col('income').mean(), \n    avg_credit = pl.col('credit').mean()\n  )\n  .sort(pl.col('avg_income'), descending=True)\n)\n```\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |>\n  group_by(gender, region) |>\n  summarize(\n    n = n(),\n    avg_income = mean(income),\n    avg_credit = mean(credit)\n  ) |> \n  arrange(desc(avg_income))\n```\n:::\n\n\n:::\n\n### Lazy evaluation\n\nBy tagging a data frame with `.lazy()`, we are asking Polars to not evaluate the code until triggered. We are also asking Polars to optimize the query for us in the underlying query engine. Before the code is triggered with something like `.collect()`, you can even see the underlying optimized query using `.explain()`.\n\nWhile there isn't a {dplyr} equivalent to share here, this is exactly what {dbplyr} does whenever writing SQL code, save for the underlying query optimization.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf = (customer_data\n  .group_by(pl.col(['gender', 'region']))\n  .agg(\n    n = pl.len(), \n    avg_income = pl.col('income').mean(), \n    avg_credit = pl.col('credit').mean()\n  )\n  .sort(pl.col('avg_income'), descending=True)\n).lazy()\n\ndf.explain()\n\ndf.collect()\n```\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_db <- customer_data |>\n  group_by(gender, region) |>\n  summarize(\n    n = n(),\n    avg_income = mean(income),\n    avg_credit = mean(credit)\n  ) |> \n  arrange(desc(avg_income))\n\ndata_db |>\n  show_query()\n\ndata_db\n```\n:::\n\n\n:::\n\n## Visualizing data\n\nThere's no way around it---visualizing data is where {ggplot2} simply reigns supreme, so much so that Posit has been investing in [plotnine](https://plotnine.org), a {ggplot2} port for Python. Maybe Posit will eventually facilitate a polyglot grammar of graphics, however in the spirit of this post, let's consider a genuinely \"Pythonic\" tool.\n\nIf NumPy is base R, [matplotlib](https://matplotlib.org) is plotting in base R. If that analogy holds, matplotlib is an acquired taste, so much so that [seaborn](https://seaborn.pydata.org) was developed to supplement matplotlib, much like pandas was developed to supplement NumPy. While we don't have a Polars-like replacement (come on Hadley, a polygplot {ggvis} is written in the stars!) we do have [seaborn.objects](https://seaborn.pydata.org/tutorial/objects_interface.html), a still-in-development module deliberately built with the consistency of the grammar of graphics in mind that also attempts to eliminate the need to invoke matplotlib for fine-tuning.\n\nAs a reminder, the grammar of graphics is a philosophical approach to visualizing data created by [Leland Wilkinson](https://link.springer.com/book/10.1007/0-387-28695-0) that inspired the creation of {ggplot2} and seaborn.objects. It's about composing a visualization a layer at a time, specifically:\n\n1. Data to visualize\n2. Mapping graphical elements to data\n3. A specific graphic representing the data and mappings\n4. Additional fine-tuning via facets, labels, scales, etc.\n\nHaving this principled approach to guide the development and consistency of a plotting approach is what distinguishes {ggplot2} and seaborn.objects. (I have often made the argument that SQL itself transcends languages and provides a kind of grammar of data manipulation.) Let's illustrate some common visualizations using seaborn.objects and {ggplot2} and gain some intuition for how they are related and divergent.\n\n### Column plots\n\nOnce again, we see the use of an alias to shorten the namespace reference. Here, the alias convention for the seaborn.objects module is `so`. We also see one of the limitations of method chaining (and thus object-oriented programming): Methods are specific to objects defined by libraries. Thus we can't directly method chain data wrangled using Polars objects to be visualized by seaborn.objects like we can pipe data from {dplyr} to {ggplot2} in R.\n\nHowever, the plot itself starts with a familiar-looking `so.Plot()` function (seaborn-objects' version of `ggplot()`) which instantiates a Plot object and specifies (1) our data and (2) the mapping between that data and graphical elements. Then with a consistency that isn't present with {ggplot2} (I'm looking at you `|>` vs. `+`), there are a set of methods applicable to the Plot object, starting with `.add()`, that can be method chained. Finally the (3) specific graphic is created with another familiar-looking `so.Bar()`, which is a specific example of an object called a Mark (seaborn-objects' version of `geom_*()`).\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport seaborn.objects as so\n\nregion_count = (customer_data\n  .group_by(pl.col('region'))\n  .agg(n = pl.len())\n)\n\n(so.Plot(region_count, x = 'region', y = 'n')\n  .add(so.Bar())\n)\n```\n:::\n\n\n\n![](figures/plot01a.png){fig-align=\"center\"}\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |> \n  count(region) |> \n  ggplot(aes(x = region, y = n)) +\n  geom_col()\n```\n:::\n\n\n:::\n\nIn certain instances we can have the necessary data wrangling done as part of the visualization. For example, in {ggplot2} we can call `geom_bar()` instead of `geom_col()` to produce the same plot while in seaborn.objects we still use the same Mark `so.Bar()` but add another object called a Stat -- in this instance `so.Hist()` to produce the sum for us.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(so.Plot(customer_data, x = 'region')\n  .add(so.Bar(), so.Hist())\n)\n```\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |> \n  ggplot(aes(x = region)) +\n  geom_bar()\n```\n:::\n\n\n:::\n\nShow more mappings and Stat options with column plots.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |> \n  count(region, college_degree) |> \n  ggplot(aes(x = region, y = n, fill = college_degree)) +\n  geom_col()\n\ncustomer_data |> \n  count(region, college_degree) |> \n  ggplot(aes(x = region, y = n, fill = college_degree)) +\n  geom_col(position = \"fill\")\n```\n:::\n\n\n:::\n\n\n### Histograms\n\nAccepting `so.Hist()` as a Stat and not a Mark, like it is in {ggplot2}, may seem awkward for the R user. However, what results in many specific geometries in {ggplot2} is reduced by the composibility of Mark and Stat in seaborn.objects. For example, a histogram also uses the `so.Hist()` Stat to bin data but uses the `so.Bars()` instead of `so.Bar()` to produce an actual histogram.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(so.Plot(customer_data, x = 'income')\n  .add(so.Bars(), so.Hist())\n)\n```\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |> \n  ggplot(aes(x = income)) +\n  geom_histogram()\n```\n:::\n\n\n:::\n\n### Scatterplots\n\nAgain we see the one-to-one correspondence of Mark objects and geometries, `so.Dot()` and `geom_point()`.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(so.Plot(customer_data, x = 'income', y = 'credit')\n  .add(so.Dot())\n)\n```\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |> \n  ggplot(aes(x = income, y = credit)) +\n  geom_point()\n```\n:::\n\n\n:::\n\nWe also see again that a specialized geometry in {ggplot2} is a combination of Mark and Stat objects in seaborn.objects, where there are arguments that can be modified within each function call.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(so.Plot(customer_data, x = 'star_rating', y = 'income')\n  .add(so.Dot(pointsize = 10, alpha = 0.5), so.Jitter(0.75))\n)\n```\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |> \n  ggplot(aes(x = star_rating, y = income)) +\n  geom_jitter(size = 3, alpha = 0.5)\n```\n:::\n\n\n:::\n\n### Line plots\n\nIt's comforting to know that working with dates is just as problematic in Python as it is in R.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(so.Plot(customer_data, x = 'review_time', y = 'star_rating')\n  .add(so.Line())\n)\n```\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |> \n  ggplot(aes(x = review_time, y = star_rating)) +\n  geom_line()\n```\n:::\n\n\n:::\n\nEven if we can't method chain between different object classes (including those from different libraries), we still need to rely on the back-and-forth between data wrangling and visualizing data.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nrating_data = (customer_data\n  .drop_nulls(pl.col('star_rating'))\n  .select(pl.col(['review_time', 'star_rating']))\n  .with_columns(\n    pl.col('review_time').str.to_date(format='%m %d, %Y').alias('review_time')\n  )\n  .with_columns(\n    pl.col('review_time').dt.year().alias('review_year')\n  )\n  .group_by('review_year')\n  .agg(pl.mean('star_rating').alias('avg_star_rating'))\n)\n\n(so.Plot(rating_data, x = 'review_year', y = 'avg_star_rating')\n  .add(so.Line())\n)\n```\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |> \n  drop_na(star_rating) |> \n  select(review_time, star_rating) |> \n  mutate(\n    review_time = mdy(review_time),\n    review_year = year(review_time)\n  ) |> \n  group_by(review_year) |> \n  summarize(avg_star_rating = mean(star_rating)) |>\n  ggplot(aes(x = review_year, y = avg_star_rating)) +\n  geom_line()\n```\n:::\n\n\n:::\n\n### Density plots\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(so.Plot(customer_data, x = 'income', color = 'gender')\n  .add(so.Area(), so.Hist())\n)\n```\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |> \n  ggplot(aes(x = income, fill = gender)) +\n  geom_density(alpha = 0.5)\n```\n:::\n\n\n:::\n\n### Facets\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |> \n  count(region, college_degree, gender) |> \n  ggplot(aes(x = region, y = n, fill = college_degree)) +\n  geom_col(position = \"fill\") +\n  facet_wrap(~ gender)\n```\n:::\n\n\n:::\n\n### Labels, scales, and colors\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |> \n  count(region, college_degree, gender) |> \n  ggplot(aes(x = region, y = n, fill = college_degree)) +\n  geom_col(position = \"fill\") +\n  facet_wrap(~ gender) +\n  labs(\n    title = \"Proportion of Customers with College Degrees by Region and Gender\",\n    subtitle = \"Based on 10,531 Customers in the CRM Database\",\n    x = \"Region\",\n    y = \"Proportion\"\n  )\n```\n:::\n\n\n:::\n\n## Modeling\n\nIf the adage is true that Python is \"the second best language for everything,\" it's machine learning that is arguably where it shines. If it's still true that new statistical models first appear in R, then it can be said that the latest and greatest in machine learning (and deep learning) is incubated in Python and Python-adjacent libraries. At a high level, this should make sense. If R is tied closely to statistics, the big tent that is Python should naturally lend itself to the learning algorithms developed in computer science.\n\n- Note on importing library functions and it being the inverse of R.\n- Simulate data.\n- Word on feature engineering? Workflows vs. Pipelines?\n\n### scikit-learn\n\n- scikit-learn is a machine learning library built on NumPy, SciPy, and matplotlib.\n- Why are we using statsmodels? For all of the assumption checking I guess?\n- `.fit()` to specify the data to train on.\n- `.predict()` to predict outcomes.\n- `.score()` to evaluate model fit.\n\n> However, the full name scikit-learn comes from its origin as a “scikit”, which is short for “SciPy Toolkit.” Scikits are extensions built on top of SciPy to provide specialized functionality. Since scikit-learn is one of these toolkits, its official name follows this convention.\n\nWhat are the modeling packages we might use for simulation-based inference? Something like R's {infer} package?\n\nWhen it comes to modeling, R is equally diverse in its approaches. However, I am partial to the consistency of the tidymodels suite of packages.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n:::\n\n\n:::\n\n### Bambi\n\nAnswer to {brms}.\n\n::: {.panel-tabset}\n### Python\n\n\n\n::: {.cell}\n\n:::\n\n\n\n### R\n\n\n\n::: {.cell}\n\n:::\n\n\n:::\n\n## Communication\n\nIn learning Python find something that provides real value quickly. This is the pitch for Shiny for Python. It has programmatic range with a consistent philosophy with reactivity. Streamlit is optimized for simple applications, like a parameterized Quarto document. Dash is built on stateless applications (i.e., independent components), like Shiny without reactive expressions where the different pieces can’t communicate. Django/Flask/FastAPI is great for large apps.\n\n- Quarto or/and Jupyter Notebooks.\n- Quarto is plain text while Jupyter has a rich-text format.\n- Jupyter Notebooks are what happen when you don't have a great IDE to work with: Functionality gets absorbed into the document type itself (e.g., an embedded kernel selector).\n- Can you render Jupyter Notebooks into Word, etc.? Yes-ish. Quarto extends this functionality via Pandoc.\n- Can you somehow run line-by-line inside of cells, or is the RTF messing with the script?\n- In a Quarto document, running Python line-by-line jumps to the end of a cell. Not an issue in a .py script.\n- Issue with getting Python and R to render in a Quarto document. Requires reticulate? Does Positron affect this?\n\nAlso, the gt and Great Tables libraries!\n\n>One thing I love to share with them is that fast.ai is built:\n>- entirely with Quarto\n>- entirely out of .ipynb files\n>\n>`quarto convert` can be used to go back and forth between .ipynb and .qmd!\n>\n>Yes, that totally works. In addition, quarto render a-notebook.ipynb works out of the box, so you don't even need to convert\n>It works both for notebooks that have been created in a different environment (and so can't be re-executed because the results are only possible to produce in, say, google colab), and for notebook that you want to re-execute at every rendering. (quarto render notebook.ipynb vs quarto render notebook.ipynb --execute) Our documentation has https://quarto.org/docs/tools/jupyter-lab.html.\n>\n>And if your folks really love the JupyterLab authoring experience, we have a JupyterLab extension that makes it a little easier to create ipynb files that work well with Quarto: https://quarto.org/docs/tools/jupyter-lab-extension.html.\n>\n>Is it possible to render an .ipynb using Quarto (after adding a yaml raw block) from a Jupyter Notebook? Or does it have to be within an IDE? I have colleagues who have used nbconvert to create pdfs via latex and they're wondering if it's possible to use Quarto's typst integration to go straight from .ipynb (not viewed in an IDE) to producing a pdf via typst. The workflow here is working in a Jupyter Notebook hosted on a Jupyter Hub.\n>\n>yes - that should be totally possible. quarto render nb.ipynb should just work.\n\n- Shiny for Python?\n- Walkthrough?\n\n## Final thoughts\n\nConclude...and follow-up with advanced ____ for the R user?\n\n- Don't be afraid to use the best of R and Python interchangeably.\n- Maintaining different styles may help you keep separate mindsets.\n- Besides RStudio, VS Code is a great code editor.\n- There are great resources online for [R Users learning Python](https://emilyriederer.netlify.app/post/py-rgo/).\n- [Polars](https://docs.pola.rs) is still under development as is [seaborn.objects](https://seaborn.pydata.org/tutorial/objects_interface.html), but both are worth following.\n- [pandas](https://wesmckinney.com/book/) is still the most popular as a complement to NumPy.\n- Master the basics so you can effectively use Copilot, ChatGPT, etc.\n\nLess snake references, more Monty Python jokes -- let's get releases named that way like R has Peanuts references.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}