{
  "hash": "ec1187f064ed854a87ffa6ef86a6d191",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"An introduction to Python for R users\"\nauthor: \"Marc Dotson\"\ndate: \"2024-10-01\"\ndescription: |\n  Most introductions to Python are written for new programmers. But if you already know another language, it's easier to learn Python using your known language as an analogy. This introduction to Python is written for R users.\ncategories:\n  - python\n  - r\nimage: figures/python.png\nslug: python-intro\nexecute:\n  eval: false\n---\n\n\n\n \nIn classes and workshops over many years, I've taught analytics using R. But in my new position, I teach analytics using Python. This introduction to Python is for R users---primarily me, though I hope it proves useful to others as well.\n\n![](figures/python-r.png){width=60% fig-align=\"center\"}\n\nThere are incredible resources in this space, and I've drawn liberally from a number of them. As an overall introduction to Python, [*Python for Data Analysis*](https://wesmckinney.com/book/) is a go-to resource and the spiritual equivalent of [*R for Data Science*](https://r4ds.hadley.nz). I also really appreciate the work in [*Python and R for the Modern Data Scientist*](https://www.amazon.com/Python-Modern-Data-Scientist-Worlds/dp/1492093408), especially for the authors' clear espousing that this isn't an either/or situation---you can (and arguably *should*) use Python and R as complements.\n\nI am especially indebted to Emily Riederer's blog series beginning with [Python Rgonomics](https://emilyriederer.netlify.app/post/py-rgo/) and subscribe to her philosophy of using tools in Python that are genuinely \"Pythonic\" while being consistent with the workflow and ergonomics of the best R has to offer. I am also grateful to extra help from Posit Conf workshop instructors and colleagues in my new position at Utah State University. Additional resources will be provided where relevant in each section.\n\n- Materials in DATA 5600, 3300, and 3500, including Pedram's YouTube?\n\n## Mindset\n\nWhen you start working with Python, it's essential that you approach it with the right mindset. R is a specialized language developed by statisticians for data analysis. Python is a *big* tent, a general programming language developed by computer scientists for many things, with only a small portion of it dedicated to data analysis.\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n### Python\n\n::: {.incremental}\n- General language\n- Developed by computer scientists\n- Obsessed with efficiency\n- Obsessed with namespacing\n- Object-oriented programming\n- Small, medium, and large data\n- Machine learning and deep learning\n- Spacing is part of the syntax\n- No single authority\n- Jupyter Notebooks\n- Inconsistent (i.e., no tidyverse)\n- \"Pythonistas\" and \"Pythonic\" code\n:::\n:::\n\n::: {.column width=\"50%\"}\n### R\n\n::: {.incremental}\n- Specialized language\n- Developed by statisticians\n- Lazy about efficiency\n- Lazy about namespacing\n- Functional programming\n- Small and medium data\n- Data wrangling and visualization\n- Spacing is for convenience\n- Dominated by Posit\n- R Markdown and Quarto\n- Consistency in the tidyverse\n- \"R Users\" and \"Tidy\" code\n:::\n:::\n\n::::\n\n::: {.callout-tip title=\"CLI\"}\nLearning to be okay with command line for installation.\n:::\n\n## Installation, IDEs, and virtual environments\n\n- pyenv and pip or uv and venv or pdm?\n- Positron or VS Code.\n- Reference my own reproducible environments post, but expand on the callouts just for Python.\n\n> When you run Python code, note the reticulate::repl_python() call. REPL means â€œread-eval-print loop,â€ which is an interactive coding environment like youâ€™re used to (the opposite would be coding that had to be compiled).\n\nThe Consoleâ€™s >>> indicates youâ€™re operating in Python.\n\n::: {.callout-tip title=\"Python with VS Code\"}\nSidebar with directions for setting up how to use Python in VS Code.\n:::\n\nPython uses **reproducible environments** (and R should as well). A reproducible environment is a lot like RStudio projects but where each project has:\n\n- A specific version of Python identified\n- Its own library of installed packages\n\nThis helps us make sure that projects work even after certain functionality gets deprecated and makes sure we don't make *big* mistakes (like **using or messing with your operating system's version of Python**).\n\nI wrote a [blog post](https://occasionaldivergences.com/posts/rep-env/) about reproducible environments in R and Python. And here's some helpful [installation instructions](https://chendaniely.github.io/python_setup/210-python_install.html) for Python specifically.\n\n## Python essentials?\n\n- [Base Python Rgonomic Patterns](https://emilyriederer.netlify.app/post/py-rgo-base/)\n- Code relies on white space not braces.\n- Everything is an object, including imported modules (a module is a single file, while a package is a collection of modules in a directory).\n- The assignment (or *binding*) operator is `=`.\n- Attached functions called *methods* use the `.` notation following the object, `obj.some_method(x, y, z)`. Note that CmdStanR adopts this syntax. For example `model$sample()` in CmdStanR is `model.sample()` in CmdStanPy.\n- Lists are the fundamental data class and are created with single square brackets.\n- Objects aren't copied needlessly, they are just bound to the same underlying data structure.\n- Many objects or *mutable*. Their contents can be changed.\n- **Remember that Python is zero-indexed.**\n- `NULL` is referred to as `None`. Other types include `str`, `bytes` (binary), `float`, `bool`, and `int`.\n\n## The PyData stack\n\nThe Python data (i.e., PyData) stack refers to the ecosystem of packages that enables data analysis.\n\n- NumPy is short for \"Numerical Python.\" It includes arrays and efficient computation across arrays.\n- pandas is short for \"panel data.\" It is built on NumPy and adds data frames for tabular or \"heterogeneous\" data and is the most popular package for data wrangling.\n- Polars is a newer package for data wrangling. It's name is an anagram of the query engine it uses (OLAP) and the language it's built in (Rust or rs).\n- matplotlib provides a foundation for plotting.\n- seaborn is built on matplotlib and integrates with pandas. seaborn.objects is a more consistent API based on grammar of graphics.\n- scikit-learn is a machine learning library built on NumPy, SciPy, and matplotlib.\n\n## Data wrangling\n\nPolars. A Rust library with Python bindings. A query engine with a data frame API on top. Something of an answer to pandas problems. The backend is in Rust instead of NumPy. Built on chaining expressions inside contexts instead of working on Series. Good with small and medium data. Everything stored in Arrow formats, including improvements for storing strings. No row indices. Lazy instead of eager evaluation, including a benefit for chaining.\n\n- Effective Polars?\n- [Polarsâ€™ Rgonomic Patterns](https://emilyriederer.netlify.app/post/py-rgo-polars/)\n- [A tidyverse R and polars Python side-by-side](https://robertmitchellv.com/blog/2022-07-r-python-side-by-side/r-python-side-by-side.html)\n- [Tidy Data Manipulation: dplyr vs pandas](https://blog.tidy-intelligence.com/posts/dplyr-vs-pandas/)\n- [Tidy Data Manipulation: dplyr vs polars](https://blog.tidy-intelligence.com/posts/dplyr-vs-polars/)\n\n### Load libraries and data\n\nPackages (i.e., libraries) and modules (i.e., a kind of sub-library) are imported with **aliases**. When you use functions from a package, you reference `package.function()`. This is like `package::function()` in R.\n\nEverything is an object. Depending on the object *type* you have access to their corresponding:\n\n- **attributes** as in `object.attribute`\n- **methods**, which are object-specific functions, as in `object.method()`\n\n::: {.panel-tabset .scrollable}\n### Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport polars as pl\nimport polars.selectors as cs\n\ncustomer_data = pl.read_csv('posts/python-intro/data/customer_data.csv')\ncustomer_data.shape\ncustomer_data.columns\n```\n:::\n\n\n\n\n### R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ncustomer_data <- read_csv(\"posts/python-intro/data/customer_data.csv\")\n\nglimpse(customer_data)\n```\n:::\n\n\n\n:::\n\n### Filter observations\n\nPolars **DataFrames** have methods that are similar to {dplyr} since they're both mirroring SQL. DataFrames are composed of columns called **Series** (i.e., equivalent to vectors). Unlike pandas DataFrames, Polars DataFrames don't have a **row index**.\n\nWe would reference column names with `data['column_name']` (like R's `data$column_name` or `data[\"column_name\"]` or just `column_name` with tidyeval), but Polars allows for `pl.col('column_name')`.\n\nWe use quotation marks for every column name.\n\n::: {.panel-tabset .scrollable}\n### Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncustomer_data.filter(pl.col('college_degree') == 'Yes')\ncustomer_data.filter(pl.col('region') != 'West')\ncustomer_data.filter(pl.col('gender') != 'Female', pl.col('income') > 70000)\n```\n:::\n\n\n\n\n### R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter(customer_data, college_degree == \"Yes\")\n\nfilter(customer_data, region != \"West\")\n\nfilter(customer_data, gender == \"Female\", income > 70000)\n```\n:::\n\n\n\n:::\n\n### Slice observations\n\nPython is **zero-indexed**. This is probably the most problematic (and very computer science-based) difference and why it's nice to avoid indexing if you can!\n\nThe parameters for Polars' `.slice()` are the start index and the length.\n\n::: {.panel-tabset .scrollable}\n### Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncustomer_data.slice(0, 5)\n```\n:::\n\n\n\n\n### R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslice(customer_data, 1:5)\n```\n:::\n\n\n\n:::\n\n### Sort observations\n\nIt can be strange at first, but **namespacing** is critical.\n\n- A function is preceded by the package name (e.g., `pl.col()`), unless you import the specific function (e.g., `from polars import col`).\n- A method is preceded by an object name of a certain type (e.g., `customer_data.sort()`).\n- Since object types are tied to packages, the chain back to the corresponding package is always present, explicitly or implicitly.\n\nNote that its `True` and `False`, not `TRUE` and `FALSE` or `true` and `false`.\n\n::: {.panel-tabset .scrollable}\n### Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncustomer_data.sort(pl.col('birth_year'))\ncustomer_data.sort(pl.col('birth_year'), descending=True)\n```\n:::\n\n\n\n\n### R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\narrange(customer_data, birth_year)\n\narrange(customer_data, desc(birth_year))\n```\n:::\n\n\n\n:::\n\n### Select variables\n\nUsing single square brackets `[ ]` creates a **list**. This is similar to creating a vector in R with `c()`. A list is a fundamental Python object type and can be turned into a Series.\n\nFunction (and method) arguments are also called **parameters**.\n\n- Some parameters are **positional** that have to be specified in the exact position.\n- Others are **keyword or named** (like in R).\n\n::: {.panel-tabset .scrollable}\n### Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncustomer_data.select(pl.col('region'), pl.col('review_text'))\ncustomer_data.select(pl.col(['region', 'review_text']))\n```\n:::\n\n\n\n\n### R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nselect(customer_data, region, review_text)\n```\n:::\n\n\n\n:::\n\n### Mutate variables\n\nPolars is actually a query language, like SQL. So it's not surprising to see methods with names that more closely mirror queries, like the `.with_columns()` method.\n\n::: {.panel-tabset .scrollable}\n### Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncustomer_data.with_columns(income = pl.col('income') / 1000)\n```\n:::\n\n\n\n\n### R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmutate(customer_data, income = income / 1000)\n```\n:::\n\n\n\n:::\n\n### Join data frames\n\nMissing values are identified as `NaN`. Series types include `str`, `bytes` (binary), `float`, `bool`, and `int`.\n\n::: {.panel-tabset .scrollable}\n### Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nstore_transactions = pl.read_csv('posts/python-intro/data/store_transactions.csv')\nstore_transactions.shape\nstore_transactions.columns\n\ncustomer_data.join(store_transactions, on='customer_id', how='left')\ncustomer_data.join(store_transactions, on='customer_id', how='inner')\n```\n:::\n\n\n\n\n### R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstore_transactions <- read_csv(\"posts/python-intro/data/store_transactions.csv\")\n\nglimpse(store_transactions)\n\nleft_join(customer_data, store_transactions, join_by(customer_id))\n\ninner_join(customer_data, store_transactions, join_by(customer_id))\n```\n:::\n\n\n\n:::\n\n### Consecutive lines of code\n\nWhile possible with Python code generally, Polars embraces writing consecutive lines of code using **method chaining**. Note that:\n\n- The entire chain needs to be surrounded with `( )`\n- Each line *starts* with `.`\n- You'll have to select and run the whole thing at once\n\n::: {.panel-tabset .scrollable}\n### Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(customer_data\n .join(store_transactions, on='customer_id', how='left')\n .filter(pl.col('region') == 'West', pl.col('feb_2005') == pl.col('feb_2005').max())\n .with_columns(age = 2024 - pl.col('birth_year'))\n .select(pl.col(['age', 'feb_2005']))\n .sort(pl.col('age'), descending=True)\n .slice(0, 1)\n)\n```\n:::\n\n\n\n\n### R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |> \n  left_join(store_transactions, join_by(customer_id)) |> \n  filter(region == \"West\", feb_2005 == max(feb_2005)) |> \n  mutate(age = 2024 - birth_year) |> \n  select(age, feb_2005) |> \n  arrange(desc(age)) |> \n  slice(1)\n```\n:::\n\n\n\n:::\n\n### Summarize discrete data\n\nThe `.agg()` method stands for *aggregate*, which is exactly what `summarize()` does in R.\n\n::: {.panel-tabset .scrollable}\n### Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(customer_data\n .group_by(pl.col('region'))\n .agg(n = pl.len())\n)\n\n(customer_data\n .group_by(pl.col(['region', 'college_degree']))\n .agg(n = pl.len())\n)\n```\n:::\n\n\n\n\n### R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |> \n  count(region)\n\ncustomer_data |> \n  count(region, college_degree)\n```\n:::\n\n\n\n:::\n\n### Summarize continuous data\n\nThis is a good example of where object-oriented programming requires a different mindset.\n\n- You might think that there is a general `mean()` function like in R, but there isn't and you'd have to load a specific package from the PyData stack and reference its namespace to activate such a function.\n- Instead, `.mean()` is a method for Polars Series and DataFrames.\n\n::: {.panel-tabset .scrollable}\n### Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(customer_data\n  .select(pl.col('income'))\n  .mean()\n)\n\n(customer_data\n  .select(pl.col(['income', 'credit']))\n  .mean()\n)\n```\n:::\n\n\n\n\n### R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |>\n  summarize(avg_income = mean(income))\n\ncustomer_data |>\n  summarize(\n    avg_income = mean(income),\n    avg_credit = mean(credit)\n  )\n```\n:::\n\n\n\n:::\n\n### Summarize discrete and continuous Data\n\nWe only use `.agg()` with `.group_by()`.\n\n::: {.panel-tabset .scrollable}\n### Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(customer_data\n .group_by(pl.col(['gender', 'region']))\n .agg(\n   n = pl.len(), \n   avg_income = pl.col('income').mean(), \n   avg_credit = pl.col('credit').mean()\n  )\n .sort(pl.col('avg_income'), descending=True)\n)\n```\n:::\n\n\n\n\n### R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomer_data |>\n  group_by(gender, region) |>\n  summarize(\n    n = n(),\n    avg_income = mean(income),\n    avg_credit = mean(credit)\n  ) |> \n  arrange(desc(avg_income))\n```\n:::\n\n\n\n:::\n\n## Visualization\n\nseaborn is a high-level interface to matplotlib that was originally focused on filling gaps in matplotlibâ€™s feature set, but now seaborn.objects is more based on the underlying grammar of graphics.\n\n- Start with a .Plot().\n- Add layers from the four families of methods: .Mark(), .Stat(), .Move(), .Scale().\n- reactable-py?\n\n- Walkthrough?\n- [Tidy Data Visualization: ggplot2 vs seaborn](https://blog.tidy-intelligence.com/posts/ggplot2-vs-seaborn/)\n- [Tidy Data Visualization: ggplot2 vs matplotlib](https://blog.tidy-intelligence.com/posts/ggplot2-vs-matplotlib/)\n- [Matplotlib](https://matplotlib.org/stable/users/explain/figure/api_interfaces.html#matplotlib-application-interfaces-apis)\n\n> The code in the blog post referenced uses the implicit interface, where you operate directly on plt.XXXXX, versus the explicit interface where you generate an axis and figure fig, ax = plt.subplots(). While using the implicit interface, Positron allows you to iterate on a single plot, which is the behavior you're seeing. There are a few options to tell matplotlib that you are operating on a different figure:\n> \n> - add plt.clf() between each plt.show() call. This will not give you the film strip in Positron, since youâ€™re essentially working on the same plot, but it will clear the plot so you donâ€™t have one mega plot ðŸ‘¾\n> - update the code to be in the explicit interface, which would consist of adding fig, ax = plt.subplots() to denote the creation of a new plot. Most anywhere you're doing something like plt.bar(...) would need to be update to ax.bar(...). You should still be able to use plt.show() to show all the figures.\n\n## Modeling\n\nscikit-learn, PyMC, etc.\n\nWhy are we using statsmodels? For all of the assumption checking I guess?\n\nWhat are the modeling packages we might use for simulation-based inference? Something like R's {infer} package?\n\nWhat could we use in-place of bambi? Does PyMC allow for a {brms} high-level use of Bayesian computation?\n\n- Walkthrough?\n\n## Communication\n\nIn learning Python find something that provides real value quickly. This is the pitch for Shiny for Python. It has programmatic range with a consistent philosophy with reactivity. Streamlit is optimized for simple applications, like a parameterized Quarto document. Dash is built on stateless applications (i.e., independent components), like Shiny without reactive expressions where the different pieces canâ€™t communicate. Django/Flask/FastAPI is great for large apps.\n\n- Quarto or/and Jupyter Notebooks.\n- Quarto is plain text while Jupyter has a rich-text format.\n- Can you render Jupyter Notebooks into Word, etc.? Yes-ish. Quarto extends this functionality via Pandoc.\n- Can you somehow run line-by-line inside of cells, or is the RTF messing with the script?\n- In a Quarto document, running Python line-by-line jumps to the end of a cell. Not an issue in a .py script.\n- Issue with getting Python and R to render in a Quarto document. Requires reticulate? Does Positron affect this?\n\nAlso, the gt and Great Tables libraries!\n\n>One thing I love to share with them is that fast.ai is built:\n>- entirely with Quarto\n>- entirely out of .ipynb files\n>\n>`quarto convert` can be used to go back and forth between .ipynb and .qmd!\n>\n>Yes, that totally works. In addition, quarto render a-notebook.ipynb works out of the box, so you don't even need to convert\n>It works both for notebooks that have been created in a different environment (and so can't be re-executed because the results are only possible to produce in, say, google colab), and for notebook that you want to re-execute at every rendering. (quarto render notebook.ipynb vs quarto render notebook.ipynb --execute) Our documentation has https://quarto.org/docs/tools/jupyter-lab.html.\n>\n>And if your folks really love the JupyterLab authoring experience, we have a JupyterLab extension that makes it a little easier to create ipynb files that work well with Quarto: https://quarto.org/docs/tools/jupyter-lab-extension.html.\n>\n>Is it possible to render an .ipynb using Quarto (after adding a yaml raw block) from a Jupyter Notebook? Or does it have to be within an IDE? I have colleagues who have used nbconvert to create pdfs via latex and they're wondering if it's possible to use Quarto's typst integration to go straight from .ipynb (not viewed in an IDE) to producing a pdf via typst. The workflow here is working in a Jupyter Notebook hosted on a Jupyter Hub.\n>\n>yes - that should be totally possible. quarto render nb.ipynb should just work.\n\n- Shiny for Python?\n- Walkthrough?\n\n## Final thoughts\n\nConclude...and follow-up with advanced ____ for the R user?\n\n- Don't be afraid to use the best of R and Python interchangeably.\n- Maintaining different styles may help you keep separate mindsets.\n- Besides RStudio, VS Code is a great code editor.\n- There are great resources online for [R Users learning Python](https://emilyriederer.netlify.app/post/py-rgo/).\n- [Polars](https://docs.pola.rs) is still under development as is [seaborn.objects](https://seaborn.pydata.org/tutorial/objects_interface.html), but both are worth following.\n- [pandas](https://wesmckinney.com/book/) is still the most popular as a complement to NumPy.\n- Master the basics so you can effectively use Copilot, ChatGPT, etc.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}